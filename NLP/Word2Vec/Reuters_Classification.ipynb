{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import logging\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reuters corpus has 90 tags\n",
      "The reuters corpus has 10788 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"The reuters corpus has {} tags\".format(len(reuters.categories())))\n",
    "print(\"The reuters corpus has {} documents\".format(len(reuters.fileids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>earn</td>\n",
       "      <td>3964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acq</td>\n",
       "      <td>2369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>money-fx</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>grain</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>crude</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categories  file_count\n",
       "21       earn        3964\n",
       "0         acq        2369\n",
       "46   money-fx         717\n",
       "26      grain         582\n",
       "17      crude         578"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = []\n",
    "file_count = []\n",
    "\n",
    "for i in reuters.categories():\n",
    "    file_count.append(len(reuters.fileids(i)))\n",
    "    categories.append(i)\n",
    "\n",
    "cats = pd.DataFrame({'categories': categories, \"file_count\": file_count}).sort_values('file_count', ascending=False)\n",
    "cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chose the second and third tags on this top tags list, since the first earn tag is most likely the highly-standardized news pieces with earnings reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select documents that only contains top two labels with most documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq', 'money-fx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_start = 1\n",
    "cat_end = 2\n",
    "category_filter = cats.iloc[cat_start:cat_end + 1, 0].values.tolist()\n",
    "category_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select fileid with the category filter\n",
    "doc_id_list = np.array(reuters.fileids(category_filter))\n",
    "doc_id_list = doc_id_list[doc_id_list != 'training/3267']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'training' in doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'test' in doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_doc is created with following document names: ['training/10' 'training/1000' 'training/10005' 'training/10018'\n",
      " 'training/10025'] ...\n"
     ]
    }
   ],
   "source": [
    "train_doc = doc_id_list[['training' in doc_id for doc_id in doc_id_list]]\n",
    "print(\"train_doc is created with following document names: {} ...\".format(train_doc[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_doc is created with following document names: ['test/14843' 'test/14849' 'test/14852' 'test/14861' 'test/14865'] ...\n"
     ]
    }
   ],
   "source": [
    "test_doc = doc_id_list[['test' in doc_id for doc_id in doc_id_list]]\n",
    "print(\"test_doc is created with following document names: {} ...\".format(test_doc[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus is created, the first line is: SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERGER Sumitomo Bank Ltd & lt ; SUMI . T > is certain to l ...\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [\" \".join([t for t in reuters.words(test_doc[t])])\n",
    "               for t in range(len(test_doc))]\n",
    "print(\"test_corpus is created, the first line is: {} ...\".format(test_corpus[0][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_corpus is created, the first line is: COMPUTER TERMINAL SYSTEMS & lt ; CPML > COMPLETES SALE Computer Terminal Systems Inc said it has com ...\n"
     ]
    }
   ],
   "source": [
    "train_corpus = [\" \".join([t for t in reuters.words(train_doc[t])])\n",
    "                for t in range(len(train_doc))]\n",
    "print(\"train_corpus is created, the first line is: {} ...\".format(train_corpus[0][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_response(dataframe, col_name):\n",
    "    \"\"\"\n",
    "    Function to load survey response from a pandas dataframe into a list\n",
    "    object that can be passed to the clean_corpus() function\n",
    "    -----PARAMETERS-----\n",
    "    dataframe: the pandas dataframe where the survey responses are stored\n",
    "    col_name: a string of the column name of the survey responses\n",
    "    -----OUTPUT-----\n",
    "    Returned object is a list of responses as strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        responses = [t[0] for t in dataframe[[col_name]].values.tolist()]\n",
    "    except (TypeError, NameError):\n",
    "        print(\"Please input string as col_name\")\n",
    "        pass\n",
    "    return responses\n",
    "\n",
    "\n",
    "def clean_corpus(texts, string_line=True, stopping=True, pos='v'):\n",
    "    \"\"\"\n",
    "    Function to clean up survey answers and return list for NLP processing\n",
    "    --------PARAMETERS---------\n",
    "    texts: list objects that contains survey response strings\n",
    "    string_line: if True, each returned survey response is a single string\n",
    "    if False, each response is a list of words in the original sequence\n",
    "    stopping: (default) if True, filter stopwords\n",
    "    pos: (default) if 'v', lemmatize input words as verbs;\n",
    "    if 'n', lemmatize input words as nouns\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    # print(\"$$$ empty cleaned created\")\n",
    "    print(\">>>> response cleaning initiated\")\n",
    "    for text in texts:\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(\"--cleaning response #{} out of {}\".format(i + 1, len(texts)))\n",
    "        try:\n",
    "            text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "            text = word_tokenize(text)\n",
    "            text = [t.lower() for t in text]\n",
    "            if stopping:\n",
    "                text = [t for t in text if t not in stop]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            text = [lemmatizer.lemmatize(t, pos=pos) for t in text]\n",
    "            # TODO: determine which lemmatizer to use for this project\n",
    "            cleaned.append(text)\n",
    "        except TypeError:\n",
    "            cleaned.append([])\n",
    "        i += 1\n",
    "    if string_line:\n",
    "        cleaned = [\" \".join(t) for t in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def get_bow(tokenized_text):\n",
    "    \"\"\"\n",
    "    Function to generate bow_list and word_freq from a tokenized_text\n",
    "    -----PARAMETER-----\n",
    "    tokenized_text should be in the form of [['a'], ['a', 'b'], ['b']] format,\n",
    "    where the object is a list of survey response, with each survey response\n",
    "    as a list of word tokens\n",
    "    -----OUTPUT-----\n",
    "    The function returns two objects\n",
    "    bow_list: a list of Counter objects with word frequency of each response\n",
    "    word_freq: a Counter object that summarizes the word frequency of the input\n",
    "    tokenized_text\n",
    "    \"\"\"\n",
    "    bow_list = []\n",
    "    word_freq = Counter()\n",
    "    for text in tokenized_text:\n",
    "        bow = Counter(text)\n",
    "        word_freq.update(text)\n",
    "        bow_list.append(bow)\n",
    "    print(\"This corpus has {} key words, and the 10 \\\n",
    "most frequent words are: {}\".format(len(word_freq.keys()), word_freq.most_common(10)))\n",
    "    return bow_list, word_freq"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create clean corpus for word2vec approach\n",
    "test_clean_string = clean_corpus(test_corpus)\n",
    "train_clean_string = clean_corpus(train_corpus)\n",
    "print('The first few words from cleaned test_clean_string is: {}'.format(test_clean_string[0][:100]))\n",
    "print('The first few words from cleaned train_clean_string is: {}'.format(train_clean_string[0][:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After the text is cleaned, I can now apply the BOW model on the corpora. The BOW is basically a frequency Counter, so I have written a function to get BOW from the corpora in my text_clean.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> response cleaning initiated\n",
      "--cleaning response #500 out of 898\n",
      ">>>> response cleaning initiated\n",
      "--cleaning response #500 out of 2186\n",
      "--cleaning response #1000 out of 2186\n",
      "--cleaning response #1500 out of 2186\n",
      "--cleaning response #2000 out of 2186\n"
     ]
    }
   ],
   "source": [
    "# create clean corpus for bow approach\n",
    "test_clean_token = clean_corpus(test_corpus, string_line=False)\n",
    "train_clean_token = clean_corpus(train_corpus, string_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 7126 key words, and the 10 most frequent words are: [('say', 2976), ('lt', 1112), ('share', 1067), ('dlrs', 921), ('company', 886), ('pct', 758), ('mln', 755), ('inc', 637), ('bank', 505), ('corp', 500)]\n",
      "This corpus has 11042 key words, and the 10 most frequent words are: [('say', 7388), ('lt', 2802), ('share', 2306), ('dlrs', 2247), ('mln', 2172), ('pct', 2051), ('bank', 1983), ('company', 1942), ('inc', 1469), ('u', 1291)]\n"
     ]
    }
   ],
   "source": [
    "# quick look at the word frequency\n",
    "test_bow, test_word_freq = get_bow(test_clean_token)\n",
    "train_bow, train_word_freq = get_bow(train_clean_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aabex  aame  aar  ab  abandon  abate  abatement  abboud  abegglen  abeles  \\\n",
      "0    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "1    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "2    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "3    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "4    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "\n",
      "     ...     zellerbach  zenex  zinn  zoete  zond  zondervan  zone  zoran  \\\n",
      "0    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "1    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "2    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "3    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "4    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "\n",
      "   zurich  zwermann  \n",
      "0     NaN       NaN  \n",
      "1     NaN       NaN  \n",
      "2     NaN       NaN  \n",
      "3     NaN       NaN  \n",
      "4     NaN       NaN  \n",
      "\n",
      "[5 rows x 7126 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(test_bow).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical way to reduce the noise in BOW models is to use dimensionality reduction methods, such as Latent Semantic Indexing (LSA), Random Projections (RP), Latent Dirichlet Allocation (LDA), or Hierachical Dirichlet Process (HDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glimpse of Word2Vec Model\n",
    "The alternative, of course, is to use the famous word2vec algorithm to generate continous numeric vectors. I will use gensim package to conduct this task. After I train the model with the reuters corpora, I get a dictionary that maps a numeric vector to each word that appears in the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-31 23:47:28,672 : INFO : collecting all words and their counts\n",
      "2018-07-31 23:47:28,675 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-31 23:47:28,755 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-07-31 23:47:28,757 : INFO : Loading a fresh vocabulary\n",
      "2018-07-31 23:47:28,842 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-07-31 23:47:28,843 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-07-31 23:47:28,969 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-07-31 23:47:28,972 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-07-31 23:47:28,974 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-07-31 23:47:29,054 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-07-31 23:47:29,056 : INFO : resetting layer weights\n",
      "2018-07-31 23:47:29,448 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-07-31 23:47:29,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-31 23:47:29,818 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-31 23:47:29,819 : INFO : EPOCH - 1 : training on 191770 raw words (169259 effective words) took 0.4s, 466436 effective words/s\n",
      "2018-07-31 23:47:30,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-31 23:47:30,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-31 23:47:30,355 : INFO : EPOCH - 2 : training on 191770 raw words (169521 effective words) took 0.5s, 319291 effective words/s\n",
      "2018-07-31 23:47:30,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-31 23:47:30,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-31 23:47:30,749 : INFO : EPOCH - 3 : training on 191770 raw words (169462 effective words) took 0.4s, 435706 effective words/s\n",
      "2018-07-31 23:47:31,128 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-31 23:47:31,139 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-31 23:47:31,140 : INFO : EPOCH - 4 : training on 191770 raw words (169494 effective words) took 0.4s, 438311 effective words/s\n",
      "2018-07-31 23:47:31,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-31 23:47:31,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-31 23:47:31,678 : INFO : EPOCH - 5 : training on 191770 raw words (169479 effective words) took 0.5s, 318222 effective words/s\n",
      "2018-07-31 23:47:31,680 : INFO : training on a 958850 raw words (847215 effective words) took 2.2s, 379889 effective words/s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set up logging for gensim training\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model, I can get the numeric representation of any word that's included in the model's dictionary, and even quickly calculate the \"similarity\" between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the vector of 'inc': [ 0.18019953  0.65239906 -1.00536036  0.07709646  0.31514305  0.19264233\n",
      " -0.41766736  0.85372269 -1.01059175 -0.40029141] ...\n",
      "Printing the similarity between 'inc' and 'love': 0.8626278351070105\n",
      "Printing the similarity between 'inc' and 'company': 0.9311575893495986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the vector of 'inc': {} ...\".format(model['inc'][:10]))\n",
    "print(\"Printing the similarity between 'inc' and 'love': {}\"\\\n",
    "      .format(model.wv.similarity('inc', 'love')))\n",
    "print(\"Printing the similarity between 'inc' and 'company': {}\"\\\n",
    "      .format(model.wv.similarity('inc', 'company')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_matrix(w2v_model, corpora_token):\n",
    "    \"\"\"\n",
    "    Function to aggregate document vector from a built gensim w2v model that\n",
    "    calculates the vector mean based on vector representation of words in the\n",
    "    document\n",
    "    -----PARAMETERS-----\n",
    "    w2v_model: a gensim.models.Word2Vec object that can return a numeric array\n",
    "        when queried with w2v_model['word']\n",
    "    corpora_token: a list of sentence in list form, e.g. [['sentence','one'],\n",
    "        ['sentence','two'],...]\n",
    "    -----OUTPUT-----\n",
    "    returned object (text_matrix) is a numpy.ndarray with the shape\n",
    "    (len(corpora_token), word_vector_length)\n",
    "    \"\"\"\n",
    "    word_vector_length = len(w2v_model[w2v_model.wv.index2word[0]])  # get word vector length\n",
    "    text_matrix = np.zeros((len(corpora_token), word_vector_length))\n",
    "    for i in range(len(corpora_token)):\n",
    "        text_vector = np.zeros(word_vector_length)\n",
    "        for j in range(len(corpora_token[i])):\n",
    "            try:\n",
    "                text_vector += w2v_model[corpora_token[i][j]]\n",
    "            except:\n",
    "                pass\n",
    "            if j == len(corpora_token[i]) - 1:\n",
    "                text_vector = text_vector / len(corpora_token[i])\n",
    "        text_matrix[i][:] = text_vector\n",
    "    return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "test_matrix = get_doc_matrix(model, test_clean_token)\n",
    "train_matrix = get_doc_matrix(model, train_clean_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0 -0.007053  0.510637 -0.444755  0.381168 -0.253832  0.218109 -0.314217   \n",
      "1  0.008513  0.542247 -0.479434  0.370381 -0.213024  0.225680 -0.335633   \n",
      "2 -0.191740  0.458929 -0.457054  0.216853 -0.238729  0.151147 -0.192088   \n",
      "3  0.001081  0.448670 -0.502514  0.181731 -0.062267  0.104628 -0.227786   \n",
      "4  0.007104  0.507678 -0.493714  0.525003 -0.315540  0.088627 -0.351352   \n",
      "\n",
      "         7         8         9     ...           90        91        92  \\\n",
      "0  0.196461 -0.272996 -0.212591    ...    -0.522528  0.112731 -0.133989   \n",
      "1  0.254908 -0.316666 -0.221621    ...    -0.550824  0.204567 -0.164425   \n",
      "2  0.324972 -0.121832 -0.005484    ...    -0.484766  0.145381 -0.258645   \n",
      "3  0.380240 -0.252435 -0.098570    ...    -0.471194  0.307452 -0.179525   \n",
      "4  0.237284 -0.152665 -0.195754    ...    -0.434458 -0.016464 -0.082804   \n",
      "\n",
      "         93        94        95        96        97        98        99  \n",
      "0 -0.333362 -0.817477  0.439270 -0.702878 -0.472154 -0.120065 -0.061308  \n",
      "1 -0.407681 -0.928697  0.456935 -0.740536 -0.477173 -0.193947 -0.094748  \n",
      "2 -0.281270 -0.841187  0.353696 -0.374057 -0.296182  0.070022  0.105566  \n",
      "3 -0.444195 -0.947076  0.344429 -0.494741 -0.328170 -0.284549 -0.145541  \n",
      "4 -0.279280 -0.874970  0.476032 -0.633496 -0.494105  0.108104 -0.032317  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(train_matrix).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0 -0.137655  0.409809 -0.397694  0.241818 -0.244117  0.136173 -0.209175   \n",
      "1 -0.234845  0.607888 -0.477836  0.497215 -0.475905  0.172352 -0.325924   \n",
      "2 -0.072341  0.438973 -0.423213  0.303832 -0.223895  0.119996 -0.248904   \n",
      "3 -0.222821  0.637043 -0.658885  0.825898 -0.585690  0.069154 -0.353872   \n",
      "4 -0.006987  0.377220 -0.432845  0.307658 -0.166488  0.087884 -0.266331   \n",
      "\n",
      "         7         8         9     ...           90        91        92  \\\n",
      "0  0.235962 -0.114298 -0.038916    ...    -0.433409  0.082815 -0.176602   \n",
      "1  0.200129  0.010578 -0.051898    ...    -0.571753 -0.140075 -0.225004   \n",
      "2  0.253012 -0.141939 -0.088440    ...    -0.449004  0.089478 -0.157954   \n",
      "3  0.354253  0.058971 -0.099296    ...    -0.447013 -0.338993 -0.233701   \n",
      "4  0.266183 -0.248923 -0.157470    ...    -0.400242  0.141753 -0.113321   \n",
      "\n",
      "         93        94        95        96        97        98        99  \n",
      "0 -0.237479 -0.728287  0.337757 -0.386834 -0.299427  0.070439  0.084740  \n",
      "1 -0.170406 -0.847466  0.475108 -0.458279 -0.429707  0.422101  0.219496  \n",
      "2 -0.286823 -0.786704  0.355640 -0.455408 -0.345129  0.003861  0.003886  \n",
      "3 -0.048467 -0.980192  0.689082 -0.619955 -0.634645  0.884878  0.313018  \n",
      "4 -0.313530 -0.803655  0.385657 -0.585678 -0.383678 -0.110662 -0.070930  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(test_matrix).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Binerizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "for i in train_labels:\n",
    "    all_labels.extend(i)\n",
    "for i in test_labels:\n",
    "    all_labels.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=list(set(all_labels)))\n",
    "train_labels_bin = mlb.fit_transform(train_labels)\n",
    "test_labels_bin = mlb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 4 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 10 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 14 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 17 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 31 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 34 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 35 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 42 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(train_matrix, train_labels_bin)\n",
    "y_pred = classifier.predict(test_matrix) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = mlb.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion = pd.DataFrame({\"Predicted\": test_labels, \"Actual\":test_labels })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[acq, copper]</td>\n",
       "      <td>[acq, copper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[dlr, money-fx]</td>\n",
       "      <td>[dlr, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>[acq, crude, nat-gas]</td>\n",
       "      <td>[acq, crude, nat-gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>[dlr, dmk, money-fx]</td>\n",
       "      <td>[dlr, dmk, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>[money-fx, rand]</td>\n",
       "      <td>[money-fx, rand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>[dlr, dmk, money-fx, yen]</td>\n",
       "      <td>[dlr, dmk, money-fx, yen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>[money-fx, nzdlr]</td>\n",
       "      <td>[money-fx, nzdlr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Actual                  Predicted\n",
       "0                        [acq]                      [acq]\n",
       "1         [interest, money-fx]       [interest, money-fx]\n",
       "2                [acq, copper]              [acq, copper]\n",
       "3         [interest, money-fx]       [interest, money-fx]\n",
       "4                        [acq]                      [acq]\n",
       "5                        [acq]                      [acq]\n",
       "6         [interest, money-fx]       [interest, money-fx]\n",
       "7                        [acq]                      [acq]\n",
       "8                        [acq]                      [acq]\n",
       "9                        [acq]                      [acq]\n",
       "10        [dlr, money-fx, yen]       [dlr, money-fx, yen]\n",
       "11        [interest, money-fx]       [interest, money-fx]\n",
       "12                       [acq]                      [acq]\n",
       "13             [dlr, money-fx]            [dlr, money-fx]\n",
       "14                       [acq]                      [acq]\n",
       "15                       [acq]                      [acq]\n",
       "16                       [acq]                      [acq]\n",
       "17                       [acq]                      [acq]\n",
       "18        [interest, money-fx]       [interest, money-fx]\n",
       "19                       [acq]                      [acq]\n",
       "20                       [acq]                      [acq]\n",
       "21                  [money-fx]                 [money-fx]\n",
       "22                       [acq]                      [acq]\n",
       "23                       [acq]                      [acq]\n",
       "24                       [acq]                      [acq]\n",
       "25                       [acq]                      [acq]\n",
       "26                       [acq]                      [acq]\n",
       "27                  [money-fx]                 [money-fx]\n",
       "28                       [acq]                      [acq]\n",
       "29                       [acq]                      [acq]\n",
       "..                         ...                        ...\n",
       "868      [acq, crude, nat-gas]      [acq, crude, nat-gas]\n",
       "869                      [acq]                      [acq]\n",
       "870                      [acq]                      [acq]\n",
       "871                      [acq]                      [acq]\n",
       "872                      [acq]                      [acq]\n",
       "873                      [acq]                      [acq]\n",
       "874                      [acq]                      [acq]\n",
       "875                      [acq]                      [acq]\n",
       "876                      [acq]                      [acq]\n",
       "877       [interest, money-fx]       [interest, money-fx]\n",
       "878                      [acq]                      [acq]\n",
       "879                 [money-fx]                 [money-fx]\n",
       "880                      [acq]                      [acq]\n",
       "881       [dlr, dmk, money-fx]       [dlr, dmk, money-fx]\n",
       "882                      [acq]                      [acq]\n",
       "883       [interest, money-fx]       [interest, money-fx]\n",
       "884       [interest, money-fx]       [interest, money-fx]\n",
       "885       [interest, money-fx]       [interest, money-fx]\n",
       "886                 [money-fx]                 [money-fx]\n",
       "887                 [money-fx]                 [money-fx]\n",
       "888           [money-fx, rand]           [money-fx, rand]\n",
       "889                      [acq]                      [acq]\n",
       "890                 [money-fx]                 [money-fx]\n",
       "891  [dlr, dmk, money-fx, yen]  [dlr, dmk, money-fx, yen]\n",
       "892                      [acq]                      [acq]\n",
       "893                      [acq]                      [acq]\n",
       "894                 [money-fx]                 [money-fx]\n",
       "895          [money-fx, nzdlr]          [money-fx, nzdlr]\n",
       "896                      [acq]                      [acq]\n",
       "897       [dlr, money-fx, yen]       [dlr, money-fx, yen]\n",
       "\n",
       "[898 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='micro')\n",
    "    recall = recall_score(test_labels, predictions, average='micro')\n",
    "    f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(test_labels, predictions, average='macro')\n",
    "    recall = recall_score(test_labels, predictions, average='macro')\n",
    "    f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    \n",
    "    precision = precision_score(test_labels, predictions, average='samples')\n",
    "    recall = recall_score(test_labels, predictions, average='samples')\n",
    "    f1 = f1_score(test_labels, predictions, average='samples')\n",
    "    print(\"samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.8842, Recall: 0.9830, F1-measure: 0.9310\n",
      "Macro-average quality numbers\n",
      "Precision: 0.0633, Recall: 0.0791, F1-measure: 0.0682\n",
      "samples-average quality numbers\n",
      "Precision: 0.9425, Recall: 0.9889, F1-measure: 0.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_pred, test_labels_bin)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
