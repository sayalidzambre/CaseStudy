{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import gensim\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import logging\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reuters corpus has 90 tags\n",
      "The reuters corpus has 10788 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"The reuters corpus has {} tags\".format(len(reuters.categories())))\n",
    "print(\"The reuters corpus has {} documents\".format(len(reuters.fileids())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>file_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>earn</td>\n",
       "      <td>3964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acq</td>\n",
       "      <td>2369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>money-fx</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>grain</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>crude</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categories  file_count\n",
       "21       earn        3964\n",
       "0         acq        2369\n",
       "46   money-fx         717\n",
       "26      grain         582\n",
       "17      crude         578"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = []\n",
    "file_count = []\n",
    "\n",
    "for i in reuters.categories():\n",
    "    file_count.append(len(reuters.fileids(i)))\n",
    "    categories.append(i)\n",
    "\n",
    "cats = pd.DataFrame({'categories': categories, \"file_count\": file_count}).sort_values('file_count', ascending=False)\n",
    "cats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chose the second and third tags on this top tags list, since the first earn tag is most likely the highly-standardized news pieces with earnings reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select documents that only contains top two labels with most documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq', 'money-fx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_start = 1\n",
    "cat_end = 2\n",
    "category_filter = cats.iloc[cat_start:cat_end + 1, 0].values.tolist()\n",
    "category_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select fileid with the category filter\n",
    "doc_id_list = np.array(reuters.fileids(category_filter))\n",
    "doc_id_list = doc_id_list[doc_id_list != 'training/3267']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'training' in doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = [reuters.categories(doc_id)for doc_id in doc_id_list if 'test' in doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_doc is created with following document names: ['training/10' 'training/1000' 'training/10005' 'training/10018'\n",
      " 'training/10025'] ...\n"
     ]
    }
   ],
   "source": [
    "train_doc = doc_id_list[['training' in doc_id for doc_id in doc_id_list]]\n",
    "print(\"train_doc is created with following document names: {} ...\".format(train_doc[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_doc is created with following document names: ['test/14843' 'test/14849' 'test/14852' 'test/14861' 'test/14865'] ...\n"
     ]
    }
   ],
   "source": [
    "test_doc = doc_id_list[['test' in doc_id for doc_id in doc_id_list]]\n",
    "print(\"test_doc is created with following document names: {} ...\".format(test_doc[0:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_corpus is created, the first line is: SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERGER Sumitomo Bank Ltd & lt ; SUMI . T > is certain to l ...\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [\" \".join([t for t in reuters.words(test_doc[t])])\n",
    "               for t in range(len(test_doc))]\n",
    "print(\"test_corpus is created, the first line is: {} ...\".format(test_corpus[0][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_corpus is created, the first line is: COMPUTER TERMINAL SYSTEMS & lt ; CPML > COMPLETES SALE Computer Terminal Systems Inc said it has com ...\n"
     ]
    }
   ],
   "source": [
    "train_corpus = [\" \".join([t for t in reuters.words(train_doc[t])])\n",
    "                for t in range(len(train_doc))]\n",
    "print(\"train_corpus is created, the first line is: {} ...\".format(train_corpus[0][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_response(dataframe, col_name):\n",
    "    \"\"\"\n",
    "    Function to load survey response from a pandas dataframe into a list\n",
    "    object that can be passed to the clean_corpus() function\n",
    "    -----PARAMETERS-----\n",
    "    dataframe: the pandas dataframe where the survey responses are stored\n",
    "    col_name: a string of the column name of the survey responses\n",
    "    -----OUTPUT-----\n",
    "    Returned object is a list of responses as strings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        responses = [t[0] for t in dataframe[[col_name]].values.tolist()]\n",
    "    except (TypeError, NameError):\n",
    "        print(\"Please input string as col_name\")\n",
    "        pass\n",
    "    return responses\n",
    "\n",
    "\n",
    "def clean_corpus(texts, string_line=True, stopping=True, pos='v'):\n",
    "    \"\"\"\n",
    "    Function to clean up survey answers and return list for NLP processing\n",
    "    --------PARAMETERS---------\n",
    "    texts: list objects that contains survey response strings\n",
    "    string_line: if True, each returned survey response is a single string\n",
    "    if False, each response is a list of words in the original sequence\n",
    "    stopping: (default) if True, filter stopwords\n",
    "    pos: (default) if 'v', lemmatize input words as verbs;\n",
    "    if 'n', lemmatize input words as nouns\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    # print(\"$$$ empty cleaned created\")\n",
    "    print(\">>>> response cleaning initiated\")\n",
    "    for text in texts:\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(\"--cleaning response #{} out of {}\".format(i + 1, len(texts)))\n",
    "        try:\n",
    "            text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "            text = word_tokenize(text)\n",
    "            text = [t.lower() for t in text]\n",
    "            if stopping:\n",
    "                text = [t for t in text if t not in stop]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            text = [lemmatizer.lemmatize(t, pos=pos) for t in text]\n",
    "            # TODO: determine which lemmatizer to use for this project\n",
    "            cleaned.append(text)\n",
    "        except TypeError:\n",
    "            cleaned.append([])\n",
    "        i += 1\n",
    "    if string_line:\n",
    "        cleaned = [\" \".join(t) for t in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def get_bow(tokenized_text):\n",
    "    \"\"\"\n",
    "    Function to generate bow_list and word_freq from a tokenized_text\n",
    "    -----PARAMETER-----\n",
    "    tokenized_text should be in the form of [['a'], ['a', 'b'], ['b']] format,\n",
    "    where the object is a list of survey response, with each survey response\n",
    "    as a list of word tokens\n",
    "    -----OUTPUT-----\n",
    "    The function returns two objects\n",
    "    bow_list: a list of Counter objects with word frequency of each response\n",
    "    word_freq: a Counter object that summarizes the word frequency of the input\n",
    "    tokenized_text\n",
    "    \"\"\"\n",
    "    bow_list = []\n",
    "    word_freq = Counter()\n",
    "    for text in tokenized_text:\n",
    "        bow = Counter(text)\n",
    "        word_freq.update(text)\n",
    "        bow_list.append(bow)\n",
    "    print(\"This corpus has {} key words, and the 10 \\\n",
    "most frequent words are: {}\".format(len(word_freq.keys()), word_freq.most_common(10)))\n",
    "    return bow_list, word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> response cleaning initiated\n",
      "--cleaning response #500 out of 898\n",
      ">>>> response cleaning initiated\n",
      "--cleaning response #500 out of 2186\n",
      "--cleaning response #1000 out of 2186\n",
      "--cleaning response #1500 out of 2186\n",
      "--cleaning response #2000 out of 2186\n",
      "The first few words from cleaned test_clean_string is: sumitomo bank aim quick recovery merger sumitomo bank ltd lt sumi certain lose status japan profitab\n",
      "The first few words from cleaned train_clean_string is: computer terminal systems lt cpml complete sale computer terminal systems inc say complete sale shar\n"
     ]
    }
   ],
   "source": [
    "# create clean corpus for word2vec approach\n",
    "test_clean_string = clean_corpus(test_corpus)\n",
    "train_clean_string = clean_corpus(train_corpus)\n",
    "print('The first few words from cleaned test_clean_string is: {}'.format(test_clean_string[0][:100]))\n",
    "print('The first few words from cleaned train_clean_string is: {}'.format(train_clean_string[0][:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After the text is cleaned, I can now apply the BOW model on the corpora. The BOW is basically a frequency Counter, so I have written a function to get BOW from the corpora in my text_clean.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> response cleaning initiated\n",
      "--cleaning response #500 out of 898\n",
      ">>>> response cleaning initiated\n",
      "--cleaning response #500 out of 2186\n",
      "--cleaning response #1000 out of 2186\n",
      "--cleaning response #1500 out of 2186\n",
      "--cleaning response #2000 out of 2186\n"
     ]
    }
   ],
   "source": [
    "# create clean corpus for bow approach\n",
    "test_clean_token = clean_corpus(test_corpus, string_line=False)\n",
    "train_clean_token = clean_corpus(train_corpus, string_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This corpus has 7126 key words, and the 10 most frequent words are: [('say', 2976), ('lt', 1112), ('share', 1067), ('dlrs', 921), ('company', 886), ('pct', 758), ('mln', 755), ('inc', 637), ('bank', 505), ('corp', 500)]\n",
      "This corpus has 11042 key words, and the 10 most frequent words are: [('say', 7388), ('lt', 2802), ('share', 2306), ('dlrs', 2247), ('mln', 2172), ('pct', 2051), ('bank', 1983), ('company', 1942), ('inc', 1469), ('u', 1291)]\n"
     ]
    }
   ],
   "source": [
    "# quick look at the word frequency\n",
    "test_bow, test_word_freq = get_bow(test_clean_token)\n",
    "train_bow, train_word_freq = get_bow(train_clean_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aabex  aame  aar  ab  abandon  abate  abatement  abboud  abegglen  abeles  \\\n",
      "0    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "1    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "2    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "3    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "4    NaN   NaN  NaN NaN      NaN    NaN        NaN     NaN       NaN     NaN   \n",
      "\n",
      "     ...     zellerbach  zenex  zinn  zoete  zond  zondervan  zone  zoran  \\\n",
      "0    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "1    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "2    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "3    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "4    ...            NaN    NaN   NaN    NaN   NaN        NaN   NaN    NaN   \n",
      "\n",
      "   zurich  zwermann  \n",
      "0     NaN       NaN  \n",
      "1     NaN       NaN  \n",
      "2     NaN       NaN  \n",
      "3     NaN       NaN  \n",
      "4     NaN       NaN  \n",
      "\n",
      "[5 rows x 7126 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(test_bow).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical way to reduce the noise in BOW models is to use dimensionality reduction methods, such as Latent Semantic Indexing (LSA), Random Projections (RP), Latent Dirichlet Allocation (LDA), or Hierachical Dirichlet Process (HDP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glimpse of Word2Vec Model\n",
    "The alternative, of course, is to use the famous word2vec algorithm to generate continous numeric vectors. I will use gensim package to conduct this task. After I train the model with the reuters corpora, I get a dictionary that maps a numeric vector to each word that appears in the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 12:11:04,559 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:11:04,562 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:11:04,670 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:11:04,672 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:11:04,751 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:11:04,752 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:11:04,882 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:11:04,884 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:11:04,886 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:11:04,981 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:11:04,983 : INFO : resetting layer weights\n",
      "2018-08-03 12:11:05,406 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:11:05,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:11:05,808 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:11:05,809 : INFO : EPOCH - 1 : training on 191770 raw words (169259 effective words) took 0.4s, 423884 effective words/s\n",
      "2018-08-03 12:11:06,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:11:06,210 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:11:06,211 : INFO : EPOCH - 2 : training on 191770 raw words (169521 effective words) took 0.4s, 426166 effective words/s\n",
      "2018-08-03 12:11:06,618 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:11:06,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:11:06,629 : INFO : EPOCH - 3 : training on 191770 raw words (169462 effective words) took 0.4s, 413172 effective words/s\n",
      "2018-08-03 12:11:07,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:11:07,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:11:07,052 : INFO : EPOCH - 4 : training on 191770 raw words (169494 effective words) took 0.4s, 405813 effective words/s\n",
      "2018-08-03 12:11:07,452 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:11:07,453 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:11:07,454 : INFO : EPOCH - 5 : training on 191770 raw words (169479 effective words) took 0.4s, 426218 effective words/s\n",
      "2018-08-03 12:11:07,454 : INFO : training on a 958850 raw words (847215 effective words) took 2.0s, 413748 effective words/s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set up logging for gensim training\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 12:14:04,568 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:14:04,571 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:14:04,679 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:14:04,680 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:14:04,768 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:14:04,770 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:14:04,928 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:14:04,930 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:14:04,931 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:14:05,020 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:14:05,021 : INFO : resetting layer weights\n",
      "2018-08-03 12:14:05,404 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:14:05,770 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:05,785 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:05,787 : INFO : EPOCH - 1 : training on 191770 raw words (169259 effective words) took 0.4s, 450485 effective words/s\n",
      "2018-08-03 12:14:06,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:06,220 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:06,221 : INFO : EPOCH - 2 : training on 191770 raw words (169521 effective words) took 0.4s, 398666 effective words/s\n",
      "2018-08-03 12:14:06,656 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:06,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:06,659 : INFO : EPOCH - 3 : training on 191770 raw words (169462 effective words) took 0.4s, 391841 effective words/s\n",
      "2018-08-03 12:14:07,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:07,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:07,276 : INFO : EPOCH - 4 : training on 191770 raw words (169494 effective words) took 0.6s, 277065 effective words/s\n",
      "2018-08-03 12:14:07,855 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:07,859 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:07,863 : INFO : EPOCH - 5 : training on 191770 raw words (169479 effective words) took 0.6s, 292408 effective words/s\n",
      "2018-08-03 12:14:07,865 : INFO : training on a 958850 raw words (847215 effective words) took 2.5s, 344393 effective words/s\n",
      "2018-08-03 12:14:07,867 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:14:07,869 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:14:07,973 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:14:07,975 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:14:08,006 : INFO : min_count=5 retains 3802 unique words (34% of original 11042, drops 7240)\n",
      "2018-08-03 12:14:08,007 : INFO : min_count=5 leaves 178964 word corpus (93% of original 191770, drops 12806)\n",
      "2018-08-03 12:14:08,076 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:14:08,078 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2018-08-03 12:14:08,080 : INFO : downsampling leaves estimated 155465 word corpus (86.9% of prior 178964)\n",
      "2018-08-03 12:14:08,122 : INFO : estimated required memory for 3802 words and 100 dimensions: 4942600 bytes\n",
      "2018-08-03 12:14:08,126 : INFO : resetting layer weights\n",
      "2018-08-03 12:14:08,275 : INFO : training model with 2 workers on 3802 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:14:08,630 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:08,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:08,641 : INFO : EPOCH - 1 : training on 191770 raw words (155335 effective words) took 0.4s, 430417 effective words/s\n",
      "2018-08-03 12:14:09,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:09,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:09,165 : INFO : EPOCH - 2 : training on 191770 raw words (155450 effective words) took 0.5s, 304404 effective words/s\n",
      "2018-08-03 12:14:09,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:09,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:09,636 : INFO : EPOCH - 3 : training on 191770 raw words (155476 effective words) took 0.5s, 334877 effective words/s\n",
      "2018-08-03 12:14:10,129 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:10,146 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:10,148 : INFO : EPOCH - 4 : training on 191770 raw words (155471 effective words) took 0.5s, 307096 effective words/s\n",
      "2018-08-03 12:14:10,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:10,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:10,663 : INFO : EPOCH - 5 : training on 191770 raw words (155437 effective words) took 0.5s, 306489 effective words/s\n",
      "2018-08-03 12:14:10,665 : INFO : training on a 958850 raw words (777169 effective words) took 2.4s, 325369 effective words/s\n",
      "2018-08-03 12:14:10,666 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:14:10,668 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:14:10,737 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:14:10,738 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:14:10,827 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:14:10,828 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:14:10,950 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:14:10,952 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:14:10,954 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:14:11,071 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:14:11,072 : INFO : resetting layer weights\n",
      "2018-08-03 12:14:11,465 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-08-03 12:14:11,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:11,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:11,945 : INFO : EPOCH - 1 : training on 191770 raw words (169519 effective words) took 0.5s, 356379 effective words/s\n",
      "2018-08-03 12:14:12,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:12,401 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:12,403 : INFO : EPOCH - 2 : training on 191770 raw words (169137 effective words) took 0.5s, 373834 effective words/s\n",
      "2018-08-03 12:14:12,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:12,932 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:12,934 : INFO : EPOCH - 3 : training on 191770 raw words (169492 effective words) took 0.5s, 323517 effective words/s\n",
      "2018-08-03 12:14:13,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:13,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:13,423 : INFO : EPOCH - 4 : training on 191770 raw words (169450 effective words) took 0.5s, 349721 effective words/s\n",
      "2018-08-03 12:14:13,894 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:13,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:13,909 : INFO : EPOCH - 5 : training on 191770 raw words (169401 effective words) took 0.5s, 352557 effective words/s\n",
      "2018-08-03 12:14:13,910 : INFO : training on a 958850 raw words (846999 effective words) took 2.4s, 346447 effective words/s\n",
      "2018-08-03 12:14:13,912 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:14:13,913 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:14:13,994 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:14:13,995 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:14:14,064 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:14:14,065 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:14:14,186 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:14:14,189 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:14:14,190 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:14:14,270 : INFO : estimated required memory for 11042 words and 300 dimensions: 32021800 bytes\n",
      "2018-08-03 12:14:14,272 : INFO : resetting layer weights\n",
      "2018-08-03 12:14:14,755 : INFO : training model with 2 workers on 11042 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:14:15,560 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:15,598 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:15,603 : INFO : EPOCH - 1 : training on 191770 raw words (169259 effective words) took 0.8s, 200818 effective words/s\n",
      "2018-08-03 12:14:16,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:16,533 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:16,535 : INFO : EPOCH - 2 : training on 191770 raw words (169521 effective words) took 0.9s, 184330 effective words/s\n",
      "2018-08-03 12:14:17,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:17,221 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:17,222 : INFO : EPOCH - 3 : training on 191770 raw words (169462 effective words) took 0.7s, 249039 effective words/s\n",
      "2018-08-03 12:14:17,838 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:17,840 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:17,841 : INFO : EPOCH - 4 : training on 191770 raw words (169494 effective words) took 0.6s, 276296 effective words/s\n",
      "2018-08-03 12:14:18,712 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:14:18,716 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:14:18,717 : INFO : EPOCH - 5 : training on 191770 raw words (169479 effective words) took 0.9s, 194788 effective words/s\n",
      "2018-08-03 12:14:18,718 : INFO : training on a 958850 raw words (847215 effective words) took 4.0s, 213797 effective words/s\n"
     ]
    }
   ],
   "source": [
    "cbow_m1 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=100)\n",
    "cbow_m2 = gensim.models.Word2Vec(train_clean_token, min_count=5, workers=2, window = 5, size=100)\n",
    "cbow_m3 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 10, size=100)\n",
    "cbow_m4 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 12:18:41,729 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:18:41,732 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:18:41,838 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:18:41,840 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:18:42,158 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:18:42,160 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:18:42,282 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:18:42,285 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:18:42,287 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:18:42,402 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:18:42,403 : INFO : resetting layer weights\n",
      "2018-08-03 12:18:42,811 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=30\n",
      "2018-08-03 12:18:43,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:18:43,807 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:18:43,808 : INFO : EPOCH - 1 : training on 191770 raw words (169433 effective words) took 1.0s, 173237 effective words/s\n",
      "2018-08-03 12:18:44,422 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:18:44,446 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:18:44,448 : INFO : EPOCH - 2 : training on 191770 raw words (169325 effective words) took 0.6s, 266821 effective words/s\n",
      "2018-08-03 12:18:45,081 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:18:45,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:18:45,107 : INFO : EPOCH - 3 : training on 191770 raw words (169562 effective words) took 0.7s, 259171 effective words/s\n",
      "2018-08-03 12:18:45,961 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:18:45,999 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:18:46,000 : INFO : EPOCH - 4 : training on 191770 raw words (169533 effective words) took 0.9s, 190867 effective words/s\n",
      "2018-08-03 12:18:46,623 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:18:46,626 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:18:46,627 : INFO : EPOCH - 5 : training on 191770 raw words (169531 effective words) took 0.6s, 272552 effective words/s\n",
      "2018-08-03 12:18:46,628 : INFO : training on a 958850 raw words (847384 effective words) took 3.8s, 222077 effective words/s\n"
     ]
    }
   ],
   "source": [
    "cbow_m5 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894432118604\n",
      "0.888915671319\n",
      "0.907082319364\n",
      "0.895234299138\n",
      "0.641843491056\n"
     ]
    }
   ],
   "source": [
    "print(cbow_m1.wv.similarity('love', 'like'))\n",
    "print(cbow_m2.wv.similarity('love', 'like'))\n",
    "print(cbow_m3.wv.similarity('love', 'like'))\n",
    "print(cbow_m4.wv.similarity('love', 'like'))\n",
    "print(cbow_m5.wv.similarity('love', 'like'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883191289861\n",
      "0.0930611731805\n",
      "0.94528804297\n",
      "0.799890008917\n"
     ]
    }
   ],
   "source": [
    "print(cbow_m1.wv.similarity('men', 'women'))\n",
    "#print(cbow_m2.wv.similarity('men', 'women'))\n",
    "print(cbow_m3.wv.similarity('men', 'women'))\n",
    "print(cbow_m4.wv.similarity('men', 'women'))\n",
    "print(cbow_m5.wv.similarity('men', 'women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 12:20:38,115 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:20:38,116 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:20:38,214 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:20:38,216 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:20:38,296 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:20:38,298 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:20:38,419 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:20:38,422 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:20:38,423 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:20:38,507 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:20:38,509 : INFO : resetting layer weights\n",
      "2018-08-03 12:20:38,904 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:20:39,932 : INFO : EPOCH 1 - PROGRESS: at 78.23% examples, 128633 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:40,138 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:40,221 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:40,223 : INFO : EPOCH - 1 : training on 191770 raw words (169259 effective words) took 1.3s, 129064 effective words/s\n",
      "2018-08-03 12:20:41,293 : INFO : EPOCH 2 - PROGRESS: at 88.29% examples, 140384 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:41,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:41,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:41,439 : INFO : EPOCH - 2 : training on 191770 raw words (169521 effective words) took 1.2s, 140446 effective words/s\n",
      "2018-08-03 12:20:42,489 : INFO : EPOCH 3 - PROGRESS: at 68.76% examples, 108990 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:42,834 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:42,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:42,864 : INFO : EPOCH - 3 : training on 191770 raw words (169462 effective words) took 1.4s, 119464 effective words/s\n",
      "2018-08-03 12:20:43,890 : INFO : EPOCH 4 - PROGRESS: at 78.23% examples, 128807 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:44,118 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:44,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:44,193 : INFO : EPOCH - 4 : training on 191770 raw words (169494 effective words) took 1.3s, 128056 effective words/s\n",
      "2018-08-03 12:20:45,274 : INFO : EPOCH 5 - PROGRESS: at 88.29% examples, 138408 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:45,334 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:45,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:45,428 : INFO : EPOCH - 5 : training on 191770 raw words (169479 effective words) took 1.2s, 137822 effective words/s\n",
      "2018-08-03 12:20:45,430 : INFO : training on a 958850 raw words (847215 effective words) took 6.5s, 129850 effective words/s\n",
      "2018-08-03 12:20:45,432 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:20:45,433 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:20:45,547 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:20:45,548 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:20:45,593 : INFO : min_count=5 retains 3802 unique words (34% of original 11042, drops 7240)\n",
      "2018-08-03 12:20:45,594 : INFO : min_count=5 leaves 178964 word corpus (93% of original 191770, drops 12806)\n",
      "2018-08-03 12:20:45,646 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:20:45,648 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2018-08-03 12:20:45,649 : INFO : downsampling leaves estimated 155465 word corpus (86.9% of prior 178964)\n",
      "2018-08-03 12:20:45,691 : INFO : estimated required memory for 3802 words and 100 dimensions: 4942600 bytes\n",
      "2018-08-03 12:20:45,692 : INFO : resetting layer weights\n",
      "2018-08-03 12:20:45,886 : INFO : training model with 2 workers on 3802 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:20:46,919 : INFO : EPOCH 1 - PROGRESS: at 78.23% examples, 117119 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:47,199 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:47,204 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:47,206 : INFO : EPOCH - 1 : training on 191770 raw words (155335 effective words) took 1.3s, 118258 effective words/s\n",
      "2018-08-03 12:20:48,243 : INFO : EPOCH 2 - PROGRESS: at 88.29% examples, 132392 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:48,310 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:48,382 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:48,387 : INFO : EPOCH - 2 : training on 191770 raw words (155450 effective words) took 1.2s, 132266 effective words/s\n",
      "2018-08-03 12:20:49,415 : INFO : EPOCH 3 - PROGRESS: at 94.56% examples, 144638 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:20:49,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:49,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:49,434 : INFO : EPOCH - 3 : training on 191770 raw words (155476 effective words) took 1.0s, 149817 effective words/s\n",
      "2018-08-03 12:20:50,465 : INFO : EPOCH 4 - PROGRESS: at 94.56% examples, 143593 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:20:50,466 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:50,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:50,533 : INFO : EPOCH - 4 : training on 191770 raw words (155471 effective words) took 1.1s, 142095 effective words/s\n",
      "2018-08-03 12:20:51,566 : INFO : EPOCH 5 - PROGRESS: at 94.56% examples, 143313 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:20:51,568 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:51,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:51,612 : INFO : EPOCH - 5 : training on 191770 raw words (155437 effective words) took 1.1s, 144822 effective words/s\n",
      "2018-08-03 12:20:51,613 : INFO : training on a 958850 raw words (777169 effective words) took 5.7s, 135728 effective words/s\n",
      "2018-08-03 12:20:51,614 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:20:51,615 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:20:51,747 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:20:51,751 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:20:51,840 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:20:51,844 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:20:51,999 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:20:52,000 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:20:52,002 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:20:52,100 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:20:52,103 : INFO : resetting layer weights\n",
      "2018-08-03 12:20:52,534 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-08-03 12:20:53,686 : INFO : EPOCH 1 - PROGRESS: at 26.08% examples, 38423 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:54,777 : INFO : EPOCH 1 - PROGRESS: at 78.23% examples, 58756 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:55,188 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:55,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:55,307 : INFO : EPOCH - 1 : training on 191770 raw words (169519 effective words) took 2.8s, 61280 effective words/s\n",
      "2018-08-03 12:20:56,479 : INFO : EPOCH 2 - PROGRESS: at 47.76% examples, 67486 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:20:57,471 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:57,609 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 73618 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:20:57,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:57,612 : INFO : EPOCH - 2 : training on 191770 raw words (169137 effective words) took 2.3s, 73511 effective words/s\n",
      "2018-08-03 12:20:58,680 : INFO : EPOCH 3 - PROGRESS: at 47.76% examples, 74308 words/s, in_qsize 2, out_qsize 1\n",
      "2018-08-03 12:20:59,584 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:20:59,708 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 81071 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:20:59,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:20:59,712 : INFO : EPOCH - 3 : training on 191770 raw words (169492 effective words) took 2.1s, 80924 effective words/s\n",
      "2018-08-03 12:21:00,743 : INFO : EPOCH 4 - PROGRESS: at 47.76% examples, 77027 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:01,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:01,814 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 80911 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:21:01,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:01,816 : INFO : EPOCH - 4 : training on 191770 raw words (169450 effective words) took 2.1s, 80818 effective words/s\n",
      "2018-08-03 12:21:02,960 : INFO : EPOCH 5 - PROGRESS: at 37.05% examples, 54057 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:03,981 : INFO : EPOCH 5 - PROGRESS: at 88.29% examples, 68983 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:04,127 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:04,251 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:04,253 : INFO : EPOCH - 5 : training on 191770 raw words (169401 effective words) took 2.4s, 69721 effective words/s\n",
      "2018-08-03 12:21:04,254 : INFO : training on a 958850 raw words (846999 effective words) took 11.7s, 72275 effective words/s\n",
      "2018-08-03 12:21:04,256 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:21:04,257 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:21:04,333 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:21:04,334 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:21:04,391 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:21:04,392 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:21:04,522 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:21:04,524 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:21:04,525 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:21:04,689 : INFO : estimated required memory for 11042 words and 300 dimensions: 32021800 bytes\n",
      "2018-08-03 12:21:04,690 : INFO : resetting layer weights\n",
      "2018-08-03 12:21:05,213 : INFO : training model with 2 workers on 11042 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-08-03 12:21:13,880 : INFO : EPOCH 1 - PROGRESS: at 4.94% examples, 1030 words/s, in_qsize 4, out_qsize 1\n",
      "2018-08-03 12:21:14,887 : INFO : EPOCH 1 - PROGRESS: at 10.25% examples, 1831 words/s, in_qsize 4, out_qsize 5\n",
      "2018-08-03 12:21:15,956 : INFO : EPOCH 1 - PROGRESS: at 73.01% examples, 11439 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:16,371 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:16,416 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:16,417 : INFO : EPOCH - 1 : training on 191770 raw words (169259 effective words) took 11.2s, 15153 effective words/s\n",
      "2018-08-03 12:21:17,866 : INFO : EPOCH 2 - PROGRESS: at 47.76% examples, 76776 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:18,830 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:18,992 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 78707 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:21:18,993 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:18,994 : INFO : EPOCH - 2 : training on 191770 raw words (169521 effective words) took 2.2s, 78620 effective words/s\n",
      "2018-08-03 12:21:20,011 : INFO : EPOCH 3 - PROGRESS: at 47.76% examples, 77877 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:21,045 : INFO : EPOCH 3 - PROGRESS: at 94.56% examples, 78458 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:21:21,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:21,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:21,174 : INFO : EPOCH - 3 : training on 191770 raw words (169462 effective words) took 2.2s, 77890 effective words/s\n",
      "2018-08-03 12:21:22,361 : INFO : EPOCH 4 - PROGRESS: at 37.05% examples, 52069 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:21:23,481 : INFO : EPOCH 4 - PROGRESS: at 68.76% examples, 49449 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:24,577 : INFO : EPOCH 4 - PROGRESS: at 94.56% examples, 47262 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:21:24,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:24,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:24,586 : INFO : EPOCH - 4 : training on 191770 raw words (169494 effective words) took 3.4s, 49751 effective words/s\n",
      "2018-08-03 12:21:25,735 : INFO : EPOCH 5 - PROGRESS: at 26.08% examples, 39162 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:26,747 : INFO : EPOCH 5 - PROGRESS: at 68.76% examples, 53268 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:27,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:27,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:27,563 : INFO : EPOCH - 5 : training on 191770 raw words (169479 effective words) took 3.0s, 57370 effective words/s\n",
      "2018-08-03 12:21:27,565 : INFO : training on a 958850 raw words (847215 effective words) took 22.3s, 37954 effective words/s\n",
      "2018-08-03 12:21:27,596 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:21:27,597 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:21:27,678 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:21:27,679 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:21:27,754 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:21:27,755 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:21:27,913 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:21:27,917 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:21:27,920 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:21:28,053 : INFO : estimated required memory for 11042 words and 300 dimensions: 32021800 bytes\n",
      "2018-08-03 12:21:28,056 : INFO : resetting layer weights\n",
      "2018-08-03 12:21:28,937 : INFO : training model with 2 workers on 11042 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=30\n",
      "2018-08-03 12:21:30,510 : INFO : EPOCH 1 - PROGRESS: at 4.94% examples, 5951 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:31,533 : INFO : EPOCH 1 - PROGRESS: at 15.28% examples, 10500 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:32,920 : INFO : EPOCH 1 - PROGRESS: at 26.08% examples, 11258 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:34,033 : INFO : EPOCH 1 - PROGRESS: at 37.15% examples, 12225 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:35,208 : INFO : EPOCH 1 - PROGRESS: at 47.44% examples, 12714 words/s, in_qsize 2, out_qsize 1\n",
      "2018-08-03 12:21:36,294 : INFO : EPOCH 1 - PROGRESS: at 58.33% examples, 13208 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:37,385 : INFO : EPOCH 1 - PROGRESS: at 73.01% examples, 14648 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:39,121 : INFO : EPOCH 1 - PROGRESS: at 88.29% examples, 14722 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:39,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:40,043 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:40,044 : INFO : EPOCH - 1 : training on 191770 raw words (169433 effective words) took 11.0s, 15348 effective words/s\n",
      "2018-08-03 12:21:41,976 : INFO : EPOCH 2 - PROGRESS: at 15.87% examples, 13789 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:44,797 : INFO : EPOCH 2 - PROGRESS: at 37.05% examples, 12954 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:46,841 : INFO : EPOCH 2 - PROGRESS: at 47.76% examples, 11612 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:48,869 : INFO : EPOCH 2 - PROGRESS: at 58.33% examples, 10906 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:21:50,899 : INFO : EPOCH 2 - PROGRESS: at 68.76% examples, 10488 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:52,905 : INFO : EPOCH 2 - PROGRESS: at 78.23% examples, 10213 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:55,004 : INFO : EPOCH 2 - PROGRESS: at 88.29% examples, 9949 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:21:56,091 : INFO : EPOCH 2 - PROGRESS: at 94.56% examples, 10004 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:21:56,093 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:21:56,990 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:21:56,992 : INFO : EPOCH - 2 : training on 191770 raw words (169325 effective words) took 16.9s, 9995 effective words/s\n",
      "2018-08-03 12:21:59,040 : INFO : EPOCH 3 - PROGRESS: at 5.31% examples, 4290 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:01,117 : INFO : EPOCH 3 - PROGRESS: at 15.28% examples, 6443 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:03,117 : INFO : EPOCH 3 - PROGRESS: at 26.08% examples, 7211 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:05,072 : INFO : EPOCH 3 - PROGRESS: at 37.15% examples, 7620 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:07,391 : INFO : EPOCH 3 - PROGRESS: at 47.44% examples, 7588 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:09,341 : INFO : EPOCH 3 - PROGRESS: at 58.33% examples, 7802 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:11,275 : INFO : EPOCH 3 - PROGRESS: at 68.76% examples, 7978 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:13,197 : INFO : EPOCH 3 - PROGRESS: at 78.23% examples, 8114 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:15,143 : INFO : EPOCH 3 - PROGRESS: at 88.29% examples, 8214 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:16,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:22:17,160 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 8413 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:22:17,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:22:17,166 : INFO : EPOCH - 3 : training on 191770 raw words (169562 effective words) took 20.2s, 8411 effective words/s\n",
      "2018-08-03 12:22:19,357 : INFO : EPOCH 4 - PROGRESS: at 5.31% examples, 4029 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:21,887 : INFO : EPOCH 4 - PROGRESS: at 15.87% examples, 5652 words/s, in_qsize 2, out_qsize 1\n",
      "2018-08-03 12:22:24,218 : INFO : EPOCH 4 - PROGRESS: at 26.08% examples, 6270 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:22:26,602 : INFO : EPOCH 4 - PROGRESS: at 37.15% examples, 6531 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:29,337 : INFO : EPOCH 4 - PROGRESS: at 47.76% examples, 6498 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:31,304 : INFO : EPOCH 4 - PROGRESS: at 58.33% examples, 6820 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:33,420 : INFO : EPOCH 4 - PROGRESS: at 68.76% examples, 7012 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:35,804 : INFO : EPOCH 4 - PROGRESS: at 78.23% examples, 7057 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:22:38,110 : INFO : EPOCH 4 - PROGRESS: at 88.29% examples, 7119 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:39,138 : INFO : EPOCH 4 - PROGRESS: at 94.56% examples, 7318 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:22:39,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:22:40,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:22:40,138 : INFO : EPOCH - 4 : training on 191770 raw words (169533 effective words) took 23.0s, 7384 effective words/s\n",
      "2018-08-03 12:22:42,262 : INFO : EPOCH 5 - PROGRESS: at 5.31% examples, 4132 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:22:44,454 : INFO : EPOCH 5 - PROGRESS: at 15.28% examples, 6143 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:46,640 : INFO : EPOCH 5 - PROGRESS: at 26.08% examples, 6777 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:49,092 : INFO : EPOCH 5 - PROGRESS: at 37.05% examples, 6876 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:22:51,697 : INFO : EPOCH 5 - PROGRESS: at 47.76% examples, 6832 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:53,879 : INFO : EPOCH 5 - PROGRESS: at 58.33% examples, 7008 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:22:55,998 : INFO : EPOCH 5 - PROGRESS: at 68.76% examples, 7182 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:22:58,106 : INFO : EPOCH 5 - PROGRESS: at 78.23% examples, 7315 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:23:00,242 : INFO : EPOCH 5 - PROGRESS: at 88.29% examples, 7413 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:23:01,383 : INFO : EPOCH 5 - PROGRESS: at 94.56% examples, 7565 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:23:01,386 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:23:02,328 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:23:02,330 : INFO : EPOCH - 5 : training on 191770 raw words (169531 effective words) took 22.2s, 7643 effective words/s\n",
      "2018-08-03 12:23:02,332 : INFO : training on a 958850 raw words (847384 effective words) took 93.4s, 9075 effective words/s\n"
     ]
    }
   ],
   "source": [
    "ng_m1 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=100, sg=1)\n",
    "ng_m2 = gensim.models.Word2Vec(train_clean_token, min_count=5, workers=2, window = 5, size=100, sg=1)\n",
    "ng_m3 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 10, size=100, sg=1)\n",
    "ng_m4 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 5, size=300, sg=1)\n",
    "ng_m5 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=300, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-03 12:27:40,282 : INFO : collecting all words and their counts\n",
      "2018-08-03 12:27:40,285 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-08-03 12:27:40,391 : INFO : collected 11042 word types from a corpus of 191770 raw words and 2186 sentences\n",
      "2018-08-03 12:27:40,393 : INFO : Loading a fresh vocabulary\n",
      "2018-08-03 12:27:40,491 : INFO : min_count=1 retains 11042 unique words (100% of original 11042, drops 0)\n",
      "2018-08-03 12:27:40,493 : INFO : min_count=1 leaves 191770 word corpus (100% of original 191770, drops 0)\n",
      "2018-08-03 12:27:40,613 : INFO : deleting the raw counts dictionary of 11042 items\n",
      "2018-08-03 12:27:40,616 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-08-03 12:27:40,617 : INFO : downsampling leaves estimated 169461 word corpus (88.4% of prior 191770)\n",
      "2018-08-03 12:27:40,714 : INFO : estimated required memory for 11042 words and 100 dimensions: 14354600 bytes\n",
      "2018-08-03 12:27:40,716 : INFO : resetting layer weights\n",
      "2018-08-03 12:27:41,106 : INFO : training model with 2 workers on 11042 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=30\n",
      "2018-08-03 12:27:42,126 : INFO : EPOCH 1 - PROGRESS: at 15.28% examples, 26144 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:43,245 : INFO : EPOCH 1 - PROGRESS: at 37.15% examples, 28801 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:27:44,287 : INFO : EPOCH 1 - PROGRESS: at 57.59% examples, 30341 words/s, in_qsize 4, out_qsize 1\n",
      "2018-08-03 12:27:45,310 : INFO : EPOCH 1 - PROGRESS: at 78.23% examples, 31297 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:46,074 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:27:46,321 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 32523 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:27:46,323 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:27:46,324 : INFO : EPOCH - 1 : training on 191770 raw words (169433 effective words) took 5.2s, 32500 effective words/s\n",
      "2018-08-03 12:27:47,397 : INFO : EPOCH 2 - PROGRESS: at 15.28% examples, 24909 words/s, in_qsize 2, out_qsize 1\n",
      "2018-08-03 12:27:48,425 : INFO : EPOCH 2 - PROGRESS: at 37.15% examples, 29353 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:49,438 : INFO : EPOCH 2 - PROGRESS: at 63.08% examples, 33808 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:50,461 : INFO : EPOCH 2 - PROGRESS: at 83.21% examples, 33920 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:51,126 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:27:51,378 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:27:51,380 : INFO : EPOCH - 2 : training on 191770 raw words (169325 effective words) took 5.0s, 33543 effective words/s\n",
      "2018-08-03 12:27:52,576 : INFO : EPOCH 3 - PROGRESS: at 15.28% examples, 22425 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:27:53,594 : INFO : EPOCH 3 - PROGRESS: at 37.05% examples, 27974 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:55,086 : INFO : EPOCH 3 - PROGRESS: at 58.33% examples, 26069 words/s, in_qsize 4, out_qsize 0\n",
      "2018-08-03 12:27:56,137 : INFO : EPOCH 3 - PROGRESS: at 83.21% examples, 29568 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:56,824 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:27:57,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:27:57,066 : INFO : EPOCH - 3 : training on 191770 raw words (169562 effective words) took 5.7s, 29898 effective words/s\n",
      "2018-08-03 12:27:58,134 : INFO : EPOCH 4 - PROGRESS: at 15.28% examples, 25064 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:27:59,266 : INFO : EPOCH 4 - PROGRESS: at 37.05% examples, 28082 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:28:00,335 : INFO : EPOCH 4 - PROGRESS: at 58.33% examples, 29522 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:28:01,503 : INFO : EPOCH 4 - PROGRESS: at 78.23% examples, 29663 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:28:02,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:28:02,548 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 30960 words/s, in_qsize 0, out_qsize 1\n",
      "2018-08-03 12:28:02,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:28:02,551 : INFO : EPOCH - 4 : training on 191770 raw words (169533 effective words) took 5.5s, 30941 effective words/s\n",
      "2018-08-03 12:28:03,570 : INFO : EPOCH 5 - PROGRESS: at 15.28% examples, 26134 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:28:04,585 : INFO : EPOCH 5 - PROGRESS: at 37.05% examples, 30331 words/s, in_qsize 2, out_qsize 1\n",
      "2018-08-03 12:28:05,604 : INFO : EPOCH 5 - PROGRESS: at 58.33% examples, 31584 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:28:06,609 : INFO : EPOCH 5 - PROGRESS: at 78.23% examples, 32422 words/s, in_qsize 3, out_qsize 0\n",
      "2018-08-03 12:28:07,672 : INFO : EPOCH 5 - PROGRESS: at 94.56% examples, 31411 words/s, in_qsize 1, out_qsize 1\n",
      "2018-08-03 12:28:07,673 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-03 12:28:07,837 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-03 12:28:07,838 : INFO : EPOCH - 5 : training on 191770 raw words (169531 effective words) took 5.3s, 32103 effective words/s\n",
      "2018-08-03 12:28:07,839 : INFO : training on a 958850 raw words (847384 effective words) took 26.7s, 31699 effective words/s\n"
     ]
    }
   ],
   "source": [
    "ng_m6 = gensim.models.Word2Vec(train_clean_token, min_count=1, workers=2, window = 30, size=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960223762102\n",
      "0.616894966563\n",
      "0.960637996297\n",
      "0.65993672376\n",
      "0.621251033744\n"
     ]
    }
   ],
   "source": [
    "print(ng_m1.wv.similarity('men', 'women'))\n",
    "#print(ng_m2.wv.similarity('men', 'women'))\n",
    "print(ng_m3.wv.similarity('men', 'women'))\n",
    "print(ng_m4.wv.similarity('men', 'women'))\n",
    "print(ng_m5.wv.similarity('men', 'women'))\n",
    "print(ng_m6.wv.similarity('men', 'women'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model, I can get the numeric representation of any word that's included in the model's dictionary, and even quickly calculate the \"similarity\" between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the vector of 'inc': [ 0.69633245  1.34032726  0.32753372 -0.28197432 -1.15196598 -0.1479332\n",
      " -1.96487784  0.59153831 -0.57587731  0.36454782] ...\n",
      "Printing the similarity between 'inc' and 'love': 0.8710930305539258\n",
      "Printing the similarity between 'inc' and 'company': 0.9289818467307422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing the vector of 'inc': {} ...\".format(model['inc'][:10]))\n",
    "print(\"Printing the similarity between 'inc' and 'love': {}\"\\\n",
    "      .format(model.wv.similarity('inc', 'love')))\n",
    "print(\"Printing the similarity between 'inc' and 'company': {}\"\\\n",
    "      .format(model.wv.similarity('inc', 'company')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95367470501715346"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man', 'women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96089887955037723"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('king', 'women')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99417811611921247"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('king', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89148801691870572"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('like', 'love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print(len(model[model.wv.index2word[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_doc_matrix(w2v_model, corpora_token):\n",
    "    \"\"\"\n",
    "    Function to aggregate document vector from a built gensim w2v model that\n",
    "    calculates the vector mean based on vector representation of words in the\n",
    "    document\n",
    "    -----PARAMETERS-----\n",
    "    w2v_model: a gensim.models.Word2Vec object that can return a numeric array\n",
    "        when queried with w2v_model['word']\n",
    "    corpora_token: a list of sentence in list form, e.g. [['sentence','one'],\n",
    "        ['sentence','two'],...]\n",
    "    -----OUTPUT-----\n",
    "    returned object (text_matrix) is a numpy.ndarray with the shape\n",
    "    (len(corpora_token), word_vector_length)\n",
    "    \"\"\"\n",
    "    word_vector_length = len(w2v_model[w2v_model.wv.index2word[0]])  # get word vector length\n",
    "    text_matrix = np.zeros((len(corpora_token), word_vector_length))\n",
    "    for i in range(len(corpora_token)):\n",
    "        text_vector = np.zeros(word_vector_length)\n",
    "        for j in range(len(corpora_token[i])):\n",
    "            try:\n",
    "                text_vector += w2v_model[corpora_token[i][j]]\n",
    "            except:\n",
    "                pass\n",
    "            if j == len(corpora_token[i]) - 1:\n",
    "                text_vector = text_vector / len(corpora_token[i])\n",
    "        text_matrix[i][:] = text_vector\n",
    "    return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_doc_vector_matrix(wv_model, corpus):\n",
    "    word_vector_length = len(wv_model[wv_model.wv.index2word[0]])\n",
    "    text_matrix = np.zeros((len(corpus),len(wv_model[wv_model.wv.index2word[0]])))\n",
    "    for doc_no in range(len(corpus)):\n",
    "        text_vector = np.zeros(word_vector_length)\n",
    "        for word_no in range(len(corpus[doc_no])):\n",
    "            try:\n",
    "                text_vector += wv_model[corpus[doc_no][word_no]]\n",
    "            except:\n",
    "                pass\n",
    "            if word_no == len(corpus[doc_no]) - 1:       # if you reach at last word of document\n",
    "                text_vector = text_vector / len(corpus[doc_no]) # Create average by dividing sum\n",
    "        text_matrix[doc_no][:] = text_vector\n",
    "    return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.17918864,  0.61735579, -0.12018448, ...,  0.10357096,\n",
       "         0.18109203,  0.03014295],\n",
       "       [ 0.1254951 ,  0.84262503, -0.25072033, ...,  0.21809811,\n",
       "         0.26649487,  0.00959609],\n",
       "       [ 0.18357663,  0.70173899, -0.04382254, ...,  0.09582808,\n",
       "         0.26255332,  0.12280407],\n",
       "       ..., \n",
       "       [ 0.17405347,  0.70252212, -0.17179839, ...,  0.16586508,\n",
       "         0.16602121,  0.01405772],\n",
       "       [ 0.14174666,  0.7925582 , -0.05796555, ...,  0.17251987,\n",
       "         0.37840594,  0.16619108],\n",
       "       [ 0.2563348 ,  0.73224282, -0.21198497, ...,  0.16326701,\n",
       "         0.07083301, -0.04412346]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_doc_vector_matrix(model, test_clean_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "test_matrix = get_doc_matrix(model, test_clean_token)\n",
    "train_matrix = get_doc_matrix(model, train_clean_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.145580  0.820419 -0.046329 -0.060312 -0.700513  0.229105 -0.652282   \n",
      "1  0.187050  0.885292 -0.019713 -0.080886 -0.761505  0.189836 -0.761482   \n",
      "2  0.260295  0.671416 -0.106457 -0.249679 -0.534678  0.074275 -0.359636   \n",
      "3  0.301887  0.761861  0.085036 -0.209179 -0.615830  0.031189 -0.667108   \n",
      "4  0.102003  0.864999 -0.055664 -0.132975 -0.734411  0.358447 -0.558825   \n",
      "\n",
      "         7         8         9     ...           90        91        92  \\\n",
      "0  0.103089 -0.037491  0.044101    ...    -0.316500 -1.095399 -0.631698   \n",
      "1  0.148969 -0.068057  0.070770    ...    -0.381748 -1.151074 -0.669432   \n",
      "2  0.282031  0.043433 -0.037342    ...    -0.121360 -0.768783 -0.464567   \n",
      "3  0.262176 -0.092785  0.097573    ...    -0.393109 -0.955765 -0.448833   \n",
      "4  0.096909  0.053771 -0.005729    ...    -0.295702 -1.291409 -0.601369   \n",
      "\n",
      "         93        94        95        96        97        98        99  \n",
      "0  0.465621 -0.179941  0.734391 -0.103288  0.172961  0.408460  0.182936  \n",
      "1  0.507514 -0.203055  0.808242 -0.132722  0.158635  0.412878  0.212760  \n",
      "2  0.081998 -0.000337  0.223813 -0.117924  0.085986  0.121583  0.047075  \n",
      "3  0.338009 -0.116816  0.607668 -0.154823 -0.020062  0.277059  0.238683  \n",
      "4  0.418068 -0.226613  0.800095 -0.016772  0.139848  0.487127  0.183986  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(train_matrix).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.179189  0.617356 -0.120184 -0.178332 -0.489537  0.119097 -0.343348   \n",
      "1  0.125495  0.842625 -0.250720 -0.206060 -0.679819  0.337556 -0.239724   \n",
      "2  0.183577  0.701739 -0.043823 -0.163661 -0.579389  0.164973 -0.443522   \n",
      "3  0.052820  1.006113 -0.273856 -0.322318 -0.781469  0.656680 -0.232314   \n",
      "4  0.180800  0.691567  0.006618 -0.131686 -0.595601  0.185493 -0.586476   \n",
      "\n",
      "         7         8         9     ...           90        91        92  \\\n",
      "0  0.208368  0.027998 -0.016479    ...    -0.134513 -0.747695 -0.448916   \n",
      "1  0.171765  0.197963 -0.133512    ...    -0.012870 -1.061079 -0.617072   \n",
      "2  0.187899  0.020348  0.005562    ...    -0.212436 -0.903332 -0.477231   \n",
      "3  0.106958  0.381380 -0.267700    ...     0.060949 -1.612309 -0.724327   \n",
      "4  0.202047 -0.048276  0.034002    ...    -0.300576 -0.977935 -0.508895   \n",
      "\n",
      "         93        94        95        96        97        98        99  \n",
      "0  0.135658 -0.023119  0.287054 -0.085705  0.103571  0.181092  0.030143  \n",
      "1  0.123487 -0.040554  0.318897 -0.045572  0.218098  0.266495  0.009596  \n",
      "2  0.248716 -0.096663  0.472568 -0.089012  0.095828  0.262553  0.122804  \n",
      "3  0.095426 -0.143987  0.565431  0.137927  0.263019  0.509031  0.043134  \n",
      "4  0.354159 -0.148126  0.635338 -0.104999  0.126920  0.320779  0.140305  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(test_matrix).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Binerizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "for i in train_labels:\n",
    "    all_labels.extend(i)\n",
    "for i in test_labels:\n",
    "    all_labels.extend(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=list(set(all_labels)))\n",
    "train_labels_bin = mlb.fit_transform(train_labels)\n",
    "test_labels_bin = mlb.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 2 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 6 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 13 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 14 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 22 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 26 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 29 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 32 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 41 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "classifier.fit(train_matrix, train_labels_bin)\n",
    "y_pred = classifier.predict(test_matrix) \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = mlb.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion = pd.DataFrame({\"Predicted\": test_labels, \"Actual\":test_labels })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[acq, copper]</td>\n",
       "      <td>[acq, copper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[dlr, money-fx]</td>\n",
       "      <td>[dlr, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>[acq, crude, nat-gas]</td>\n",
       "      <td>[acq, crude, nat-gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>[dlr, dmk, money-fx]</td>\n",
       "      <td>[dlr, dmk, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>[money-fx, rand]</td>\n",
       "      <td>[money-fx, rand]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>[dlr, dmk, money-fx, yen]</td>\n",
       "      <td>[dlr, dmk, money-fx, yen]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>[money-fx]</td>\n",
       "      <td>[money-fx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>[money-fx, nzdlr]</td>\n",
       "      <td>[money-fx, nzdlr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>[acq]</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "      <td>[dlr, money-fx, yen]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>898 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Actual                  Predicted\n",
       "0                        [acq]                      [acq]\n",
       "1         [interest, money-fx]       [interest, money-fx]\n",
       "2                [acq, copper]              [acq, copper]\n",
       "3         [interest, money-fx]       [interest, money-fx]\n",
       "4                        [acq]                      [acq]\n",
       "5                        [acq]                      [acq]\n",
       "6         [interest, money-fx]       [interest, money-fx]\n",
       "7                        [acq]                      [acq]\n",
       "8                        [acq]                      [acq]\n",
       "9                        [acq]                      [acq]\n",
       "10        [dlr, money-fx, yen]       [dlr, money-fx, yen]\n",
       "11        [interest, money-fx]       [interest, money-fx]\n",
       "12                       [acq]                      [acq]\n",
       "13             [dlr, money-fx]            [dlr, money-fx]\n",
       "14                       [acq]                      [acq]\n",
       "15                       [acq]                      [acq]\n",
       "16                       [acq]                      [acq]\n",
       "17                       [acq]                      [acq]\n",
       "18        [interest, money-fx]       [interest, money-fx]\n",
       "19                       [acq]                      [acq]\n",
       "20                       [acq]                      [acq]\n",
       "21                  [money-fx]                 [money-fx]\n",
       "22                       [acq]                      [acq]\n",
       "23                       [acq]                      [acq]\n",
       "24                       [acq]                      [acq]\n",
       "25                       [acq]                      [acq]\n",
       "26                       [acq]                      [acq]\n",
       "27                  [money-fx]                 [money-fx]\n",
       "28                       [acq]                      [acq]\n",
       "29                       [acq]                      [acq]\n",
       "..                         ...                        ...\n",
       "868      [acq, crude, nat-gas]      [acq, crude, nat-gas]\n",
       "869                      [acq]                      [acq]\n",
       "870                      [acq]                      [acq]\n",
       "871                      [acq]                      [acq]\n",
       "872                      [acq]                      [acq]\n",
       "873                      [acq]                      [acq]\n",
       "874                      [acq]                      [acq]\n",
       "875                      [acq]                      [acq]\n",
       "876                      [acq]                      [acq]\n",
       "877       [interest, money-fx]       [interest, money-fx]\n",
       "878                      [acq]                      [acq]\n",
       "879                 [money-fx]                 [money-fx]\n",
       "880                      [acq]                      [acq]\n",
       "881       [dlr, dmk, money-fx]       [dlr, dmk, money-fx]\n",
       "882                      [acq]                      [acq]\n",
       "883       [interest, money-fx]       [interest, money-fx]\n",
       "884       [interest, money-fx]       [interest, money-fx]\n",
       "885       [interest, money-fx]       [interest, money-fx]\n",
       "886                 [money-fx]                 [money-fx]\n",
       "887                 [money-fx]                 [money-fx]\n",
       "888           [money-fx, rand]           [money-fx, rand]\n",
       "889                      [acq]                      [acq]\n",
       "890                 [money-fx]                 [money-fx]\n",
       "891  [dlr, dmk, money-fx, yen]  [dlr, dmk, money-fx, yen]\n",
       "892                      [acq]                      [acq]\n",
       "893                      [acq]                      [acq]\n",
       "894                 [money-fx]                 [money-fx]\n",
       "895          [money-fx, nzdlr]          [money-fx, nzdlr]\n",
       "896                      [acq]                      [acq]\n",
       "897       [dlr, money-fx, yen]       [dlr, money-fx, yen]\n",
       "\n",
       "[898 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='micro')\n",
    "    recall = recall_score(test_labels, predictions, average='micro')\n",
    "    f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    print(\"Micro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "    precision = precision_score(test_labels, predictions, average='macro')\n",
    "    recall = recall_score(test_labels, predictions, average='macro')\n",
    "    f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    print(\"Macro-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "    \n",
    "    precision = precision_score(test_labels, predictions, average='samples')\n",
    "    recall = recall_score(test_labels, predictions, average='samples')\n",
    "    f1 = f1_score(test_labels, predictions, average='samples')\n",
    "    print(\"samples-average quality numbers\")\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average quality numbers\n",
      "Precision: 0.8833, Recall: 0.9851, F1-measure: 0.9314\n",
      "Macro-average quality numbers\n",
      "Precision: 0.0627, Recall: 0.0806, F1-measure: 0.0681\n",
      "samples-average quality numbers\n",
      "Precision: 0.9421, Recall: 0.9900, F1-measure: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/sayali/anaconda3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_pred, test_labels_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(train_clean_string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-02 20:50:54,522 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-08-02 20:50:54,594 : INFO : collecting all words and their counts\n",
      "2018-08-02 20:50:54,596 : WARNING : Each 'words' should be a list of words (usually unicode strings). First 'words' here is instead plain <class 'str'>.\n",
      "2018-08-02 20:50:54,600 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-08-02 20:50:54,909 : INFO : collected 27 word types and 2186 unique tags from a corpus of 2186 examples and 1305589 words\n",
      "2018-08-02 20:50:54,911 : INFO : Loading a fresh vocabulary\n",
      "2018-08-02 20:50:54,913 : INFO : min_count=1 retains 27 unique words (100% of original 27, drops 0)\n",
      "2018-08-02 20:50:54,915 : INFO : min_count=1 leaves 1305589 word corpus (100% of original 1305589, drops 0)\n",
      "2018-08-02 20:50:54,922 : INFO : deleting the raw counts dictionary of 27 items\n",
      "2018-08-02 20:50:54,924 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2018-08-02 20:50:54,925 : INFO : downsampling leaves estimated 223908 word corpus (17.2% of prior 1305589)\n",
      "2018-08-02 20:50:54,927 : INFO : estimated required memory for 27 words and 5 dimensions: 58300 bytes\n",
      "2018-08-02 20:50:54,928 : INFO : resetting layer weights\n",
      "2018-08-02 20:50:55,054 : INFO : training model with 4 workers on 27 vocabulary and 5 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2018-08-02 20:50:55,985 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-02 20:50:55,993 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-02 20:50:56,000 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-02 20:50:56,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-02 20:50:56,003 : INFO : EPOCH - 1 : training on 1305589 raw words (225776 effective words) took 0.9s, 239910 effective words/s\n",
      "2018-08-02 20:50:57,019 : INFO : EPOCH 2 - PROGRESS: at 90.48% examples, 201299 words/s, in_qsize 7, out_qsize 0\n",
      "2018-08-02 20:50:57,121 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-02 20:50:57,135 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-02 20:50:57,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-02 20:50:57,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-02 20:50:57,150 : INFO : EPOCH - 2 : training on 1305589 raw words (225821 effective words) took 1.1s, 197702 effective words/s\n",
      "2018-08-02 20:50:58,165 : INFO : EPOCH 3 - PROGRESS: at 95.88% examples, 215582 words/s, in_qsize 5, out_qsize 2\n",
      "2018-08-02 20:50:58,206 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-02 20:50:58,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-02 20:50:58,211 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-02 20:50:58,213 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-02 20:50:58,216 : INFO : EPOCH - 3 : training on 1305589 raw words (226527 effective words) took 1.1s, 215258 effective words/s\n",
      "2018-08-02 20:50:59,103 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-02 20:50:59,112 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-02 20:50:59,115 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-02 20:50:59,117 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-02 20:50:59,119 : INFO : EPOCH - 4 : training on 1305589 raw words (225860 effective words) took 0.9s, 252299 effective words/s\n",
      "2018-08-02 20:51:00,039 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-08-02 20:51:00,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-08-02 20:51:00,049 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-08-02 20:51:00,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-08-02 20:51:00,051 : INFO : EPOCH - 5 : training on 1305589 raw words (226238 effective words) took 0.9s, 244285 effective words/s\n",
      "2018-08-02 20:51:00,051 : INFO : training on a 6527945 raw words (1130222 effective words) took 5.0s, 226268 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06562854, -0.0525164 ,  0.05182675, -0.02825312, -0.03992648], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"system\", \"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-02 20:55:18,084 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot compute similarity with no input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-cac6dec5e015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sayali/anaconda3/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, indexer)\u001b[0m\n\u001b[1;32m   1319\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc '%s' not in trained set\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot compute similarity with no input"
     ]
    }
   ],
   "source": [
    "model.docvecs.most_similar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
