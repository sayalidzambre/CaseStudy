{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/usr/lib/python2.7/dist-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import scan, bulk\n",
    "from elasticsearch import ElasticsearchException\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#from normalization import normalize_corpus,review_to_sentences\n",
    "from utils import build_feature_matrix\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################################\n",
      "Execution Started............\n"
     ]
    }
   ],
   "source": [
    "start_script = time.time()\n",
    "print ''\n",
    "print \"#######################################################################################\"\n",
    "print \"Execution Started............\"\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import DocType, String, Date, Integer\n",
    "from elasticsearch_dsl.connections import connections\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "import requests\n",
    "\n",
    "index = 'ccthinclient'\n",
    "doc_type = 'thin_client'\n",
    "\n",
    "elasticsearch_host = '35.167.139.79:9200'\n",
    "elastic_url = 'http://35.167.139.79:9200/_bulk?pretty=true'\n",
    "body = {\n",
    "    \"query\": {\n",
    "                    \"exists\": {\n",
    "                        \"field\": \"pathforsearch\"\n",
    "                    }\n",
    "             }\n",
    "    }\n",
    "\n",
    "body = {\n",
    "            \"query\": {\n",
    "                \"match\":{\n",
    "                        \"taggedbyadmin\":\"false\"\n",
    "                }\n",
    "            }\n",
    "    }\n",
    "body = { \n",
    "    \"query\" : {\n",
    "        \"bool\":{\n",
    "            \"filter\":{\n",
    "                \"match\" : {\"taggedbyadmin.keyword\":\"\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "model_file_name = \"cybercentric_model.pkl\"\n",
    "\n",
    "top_n_tags = 3\n",
    "\n",
    "single_time_execute_no_documents = 100\n",
    "\n",
    "threshold_cosine = 0.1\n",
    "\n",
    "#elasticsearch_host = '52.24.198.221:8080'\n",
    "#elastic_url = 'http://52.24.198.221:8080/_bulk?pretty=true'\n",
    "\n",
    "# Define a default Elasticsearch client\n",
    "client = connections.create_connection(hosts=[elasticsearch_host])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if not os.path.exists(model_file_name):\n",
    "        print(\"Pleae run model before classifying documents\")\n",
    "    else :\n",
    "        with open(model_file_name, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            vectorizer = model.named_steps['vectorizer']\n",
    "            labels = model.labels_\n",
    "            tfidf_features = model.features_\n",
    "            model_build_time = model.timestamp_\n",
    "except IOError as e:\n",
    "    print (e.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test documents :  58806\n"
     ]
    }
   ],
   "source": [
    "def get_no_of_docs_from_elasticsearch(field):\n",
    "    body = {\n",
    "            \"query\": {\n",
    "                \"exists\": {\n",
    "                    \"field\": field\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    s = Search.from_dict(body)\n",
    "    s = s.index(index)\n",
    "    s = s.doc_type(doc_type)\n",
    "    s.using(client)\n",
    "    return s.count()\n",
    "\n",
    "test_no = get_no_of_docs_from_elasticsearch('pathforsearch')\n",
    "print \"Total test documents : \", test_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class TagScore:\n",
    "    def __init__(self, tag, sim_score):\n",
    "        self.tag = tag\n",
    "        self.sim_score = sim_score\n",
    "\n",
    "\n",
    "def get_top_tag(top5):\n",
    "    toptag_dir = {}\n",
    "    if len(top5) > 0:\n",
    "        for row in top5:\n",
    "            if isinstance(row, TagScore):\n",
    "                for tag in row.tag:\n",
    "                    if(tag in toptag_dir and \n",
    "                       toptag_dir[str(tag)] > row.sim_score):\n",
    "                        pass\n",
    "                    else :\n",
    "                        toptag_dir[str(tag)] = row.sim_score\n",
    "    return toptag_dir\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score\n",
    "\n",
    "\n",
    "def elasticsearch_bulk_update(tags_scores) :\n",
    "    start_update  = time.time()\n",
    "    try:\n",
    "        print('Updating elasticsearch data with Cosine Distance.......')\n",
    "        response = requests.post(elastic_url, data=''.join(tags_scores))\n",
    "        print('elasticsearch data updated in ', time.time() - start_update, 'sec')\n",
    "    except Exception as e:\n",
    "        print(\"Error in bulk update to server %s : \"%elastic_url, e.message) \n",
    "        response = None\n",
    "    return response\n",
    "\n",
    "def run_classification(raw_text):\n",
    "    texts = []\n",
    "    ids = []\n",
    "    for row in raw_text:\n",
    "        texts.append(row['preprocessed_text'])\n",
    "        ids.append(row['file_id'])\n",
    "        \n",
    "    query_docs_tfidf = vectorizer.transform(texts)\n",
    "    print 'Document Similarity Analysis using Cosine Similarity'\n",
    "    print('index', '||', 'tagscore', '||', 'top_tag' , '||', 'actual_tag')\n",
    "    start_cosine = time.time()\n",
    "    tag_pred_cos = []\n",
    "    json_update = []\n",
    "    for index, doc_id in enumerate(ids):\n",
    "\n",
    "        doc_tfidf = query_docs_tfidf[index]\n",
    "        top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                                 tfidf_features,\n",
    "                                                 top_n=top_n_tags)\n",
    "        top5 = []\n",
    "        sim_scores = []\n",
    "        for doc_index, sim_score in top_similar_docs:\n",
    "            if sim_score > threshold_cosine :\n",
    "                top5.append(TagScore(labels[doc_index], sim_score))\n",
    "\n",
    "\n",
    "        tagsscore_dir = get_top_tag(top5)\n",
    "\n",
    "        tag_pred_cos.append(tagsscore_dir)\n",
    "        key = ids[index]\n",
    "        tags = str(tagsscore_dir.keys()).replace('\\'', '\"')\n",
    "        tagscore = str(tagsscore_dir).replace('\\'', '\"')\n",
    "        predict_time = str(time.time())\n",
    "        print(index, '|', tagscore, '|', tags, '|')\n",
    "        json_update.append('''\n",
    "        { \\\"update\\\": { \\\"_index\\\": \\\"ccthinclient\\\", \\\"_type\\\": \\\"thin_client\\\", \\\"_id\\\": \\\"%s\\\", \\\"_retry_on_conflict\\\" : 1} }\n",
    "        { \\\"script\\\":{  \\\"inline\\\": \\\"ctx._source.tagscore=params.tagscore;ctx._source.tags=params.tags;ctx._source.predict_time=params.predict_time\\\", \\\"params\\\":{ \\\"tagscore\\\":%s, \\\"tags\\\":%s, \\\"predict_time\\\":%s }   } }'''%(key, tagscore, tags, predict_time))\n",
    "\n",
    "    json_update.append('''\\n''')    \n",
    "    \n",
    "    print 'Time taken to predict data cosine : ', time.time() - start_cosine\n",
    "    return json_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test documents.........\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "('index', '||', 'tagscore', '||', 'top_tag', '||', 'actual_tag')\n",
      "(0, '|', '{\"mobile\": 0.627}', '|', '[\"mobile\"]', '|')\n",
      "(1, '|', '{\"mobile\": 0.104}', '|', '[\"mobile\"]', '|')\n",
      "(2, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(3, '|', '{}', '|', '[]', '|')\n",
      "(4, '|', '{}', '|', '[]', '|')\n",
      "(5, '|', '{}', '|', '[]', '|')\n",
      "(6, '|', '{\"mobile\": 0.36}', '|', '[\"mobile\"]', '|')\n",
      "(7, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(8, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(9, '|', '{}', '|', '[]', '|')\n",
      "(10, '|', '{}', '|', '[]', '|')\n",
      "(11, '|', '{}', '|', '[]', '|')\n",
      "(12, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(13, '|', '{}', '|', '[]', '|')\n",
      "(14, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(15, '|', '{}', '|', '[]', '|')\n",
      "(16, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(17, '|', '{\"mobile\": 0.108}', '|', '[\"mobile\"]', '|')\n",
      "(18, '|', '{\"mobile\": 0.122}', '|', '[\"mobile\"]', '|')\n",
      "(19, '|', '{\"mobile\": 0.104}', '|', '[\"mobile\"]', '|')\n",
      "(20, '|', '{}', '|', '[]', '|')\n",
      "(21, '|', '{}', '|', '[]', '|')\n",
      "(22, '|', '{}', '|', '[]', '|')\n",
      "(23, '|', '{}', '|', '[]', '|')\n",
      "(24, '|', '{}', '|', '[]', '|')\n",
      "(25, '|', '{\"mobile\": 0.139}', '|', '[\"mobile\"]', '|')\n",
      "(26, '|', '{\"mobile\": 0.122}', '|', '[\"mobile\"]', '|')\n",
      "(27, '|', '{}', '|', '[]', '|')\n",
      "(28, '|', '{}', '|', '[]', '|')\n",
      "(29, '|', '{}', '|', '[]', '|')\n",
      "(30, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(31, '|', '{}', '|', '[]', '|')\n",
      "(32, '|', '{\"mobile\": 0.627}', '|', '[\"mobile\"]', '|')\n",
      "(33, '|', '{}', '|', '[]', '|')\n",
      "(34, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(35, '|', '{}', '|', '[]', '|')\n",
      "(36, '|', '{}', '|', '[]', '|')\n",
      "(37, '|', '{}', '|', '[]', '|')\n",
      "(38, '|', '{}', '|', '[]', '|')\n",
      "(39, '|', '{}', '|', '[]', '|')\n",
      "(40, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(41, '|', '{}', '|', '[]', '|')\n",
      "(42, '|', '{}', '|', '[]', '|')\n",
      "(43, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(44, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(45, '|', '{\"mobile\": 0.104}', '|', '[\"mobile\"]', '|')\n",
      "(46, '|', '{}', '|', '[]', '|')\n",
      "(47, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(48, '|', '{\"mobile\": 0.197}', '|', '[\"mobile\"]', '|')\n",
      "(49, '|', '{}', '|', '[]', '|')\n",
      "(50, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(51, '|', '{\"mobile\": 0.153}', '|', '[\"mobile\"]', '|')\n",
      "(52, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(53, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(54, '|', '{}', '|', '[]', '|')\n",
      "(55, '|', '{\"mobile\": 0.108}', '|', '[\"mobile\"]', '|')\n",
      "(56, '|', '{}', '|', '[]', '|')\n",
      "(57, '|', '{}', '|', '[]', '|')\n",
      "(58, '|', '{\"mobile\": 0.108}', '|', '[\"mobile\"]', '|')\n",
      "(59, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(60, '|', '{}', '|', '[]', '|')\n",
      "(61, '|', '{\"mobile\": 0.142}', '|', '[\"mobile\"]', '|')\n",
      "(62, '|', '{}', '|', '[]', '|')\n",
      "(63, '|', '{}', '|', '[]', '|')\n",
      "(64, '|', '{}', '|', '[]', '|')\n",
      "(65, '|', '{}', '|', '[]', '|')\n",
      "(66, '|', '{}', '|', '[]', '|')\n",
      "(67, '|', '{}', '|', '[]', '|')\n",
      "(68, '|', '{\"mobile\": 0.13}', '|', '[\"mobile\"]', '|')\n",
      "(69, '|', '{}', '|', '[]', '|')\n",
      "(70, '|', '{}', '|', '[]', '|')\n",
      "(71, '|', '{\"mobile\": 0.25}', '|', '[\"mobile\"]', '|')\n",
      "(72, '|', '{}', '|', '[]', '|')\n",
      "(73, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(74, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(75, '|', '{}', '|', '[]', '|')\n",
      "(76, '|', '{}', '|', '[]', '|')\n",
      "(77, '|', '{}', '|', '[]', '|')\n",
      "(78, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(79, '|', '{}', '|', '[]', '|')\n",
      "(80, '|', '{\"mobile\": 0.139}', '|', '[\"mobile\"]', '|')\n",
      "(81, '|', '{}', '|', '[]', '|')\n",
      "(82, '|', '{\"mobile\": 0.103}', '|', '[\"mobile\"]', '|')\n",
      "(83, '|', '{\"mobile\": 0.103}', '|', '[\"mobile\"]', '|')\n",
      "(84, '|', '{}', '|', '[]', '|')\n",
      "(85, '|', '{\"mobile\": 0.131}', '|', '[\"mobile\"]', '|')\n",
      "(86, '|', '{\"mobile\": 0.131}', '|', '[\"mobile\"]', '|')\n",
      "(87, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(88, '|', '{}', '|', '[]', '|')\n",
      "(89, '|', '{}', '|', '[]', '|')\n",
      "(90, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(91, '|', '{\"mobile\": 0.105}', '|', '[\"mobile\"]', '|')\n",
      "(92, '|', '{\"mobile\": 0.105}', '|', '[\"mobile\"]', '|')\n",
      "(93, '|', '{}', '|', '[]', '|')\n",
      "(94, '|', '{}', '|', '[]', '|')\n",
      "(95, '|', '{}', '|', '[]', '|')\n",
      "(96, '|', '{\"mobile\": 0.204}', '|', '[\"mobile\"]', '|')\n",
      "(97, '|', '{}', '|', '[]', '|')\n",
      "(98, '|', '{}', '|', '[]', '|')\n",
      "(99, '|', '{\"mobile\": 0.133}', '|', '[\"mobile\"]', '|')\n",
      "Time taken to predict data cosine :  0.108453989029\n",
      "Updating elasticsearch data with Cosine Distance.......\n",
      "('elasticsearch data updated in ', 2.883341073989868, 'sec')\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "('index', '||', 'tagscore', '||', 'top_tag', '||', 'actual_tag')\n",
      "(0, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(1, '|', '{}', '|', '[]', '|')\n",
      "(2, '|', '{}', '|', '[]', '|')\n",
      "(3, '|', '{\"mobile\": 0.149}', '|', '[\"mobile\"]', '|')\n",
      "(4, '|', '{\"mobile\": 0.25}', '|', '[\"mobile\"]', '|')\n",
      "(5, '|', '{}', '|', '[]', '|')\n",
      "(6, '|', '{}', '|', '[]', '|')\n",
      "(7, '|', '{}', '|', '[]', '|')\n",
      "(8, '|', '{\"mobile\": 0.153}', '|', '[\"mobile\"]', '|')\n",
      "(9, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(10, '|', '{\"mobile\": 0.122}', '|', '[\"mobile\"]', '|')\n",
      "(11, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(12, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(13, '|', '{}', '|', '[]', '|')\n",
      "(14, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(15, '|', '{\"mobile\": 0.122}', '|', '[\"mobile\"]', '|')\n",
      "(16, '|', '{}', '|', '[]', '|')\n",
      "(17, '|', '{}', '|', '[]', '|')\n",
      "(18, '|', '{}', '|', '[]', '|')\n",
      "(19, '|', '{\"mobile\": 0.13}', '|', '[\"mobile\"]', '|')\n",
      "(20, '|', '{}', '|', '[]', '|')\n",
      "(21, '|', '{}', '|', '[]', '|')\n",
      "(22, '|', '{\"mobile\": 0.25}', '|', '[\"mobile\"]', '|')\n",
      "(23, '|', '{}', '|', '[]', '|')\n",
      "(24, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(25, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(26, '|', '{}', '|', '[]', '|')\n",
      "(27, '|', '{}', '|', '[]', '|')\n",
      "(28, '|', '{}', '|', '[]', '|')\n",
      "(29, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(30, '|', '{}', '|', '[]', '|')\n",
      "(31, '|', '{\"mobile\": 0.139}', '|', '[\"mobile\"]', '|')\n",
      "(32, '|', '{}', '|', '[]', '|')\n",
      "(33, '|', '{\"mobile\": 0.103}', '|', '[\"mobile\"]', '|')\n",
      "(34, '|', '{\"mobile\": 0.103}', '|', '[\"mobile\"]', '|')\n",
      "(35, '|', '{}', '|', '[]', '|')\n",
      "(36, '|', '{\"mobile\": 0.131}', '|', '[\"mobile\"]', '|')\n",
      "(37, '|', '{\"mobile\": 0.131}', '|', '[\"mobile\"]', '|')\n",
      "(38, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(39, '|', '{}', '|', '[]', '|')\n",
      "(40, '|', '{}', '|', '[]', '|')\n",
      "(41, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(42, '|', '{}', '|', '[]', '|')\n",
      "(43, '|', '{}', '|', '[]', '|')\n",
      "(44, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(45, '|', '{}', '|', '[]', '|')\n",
      "(46, '|', '{}', '|', '[]', '|')\n",
      "(47, '|', '{}', '|', '[]', '|')\n",
      "(48, '|', '{}', '|', '[]', '|')\n",
      "(49, '|', '{}', '|', '[]', '|')\n",
      "(50, '|', '{}', '|', '[]', '|')\n",
      "(51, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(52, '|', '{\"mobile\": 0.102}', '|', '[\"mobile\"]', '|')\n",
      "(53, '|', '{}', '|', '[]', '|')\n",
      "(54, '|', '{\"mobile\": 0.142}', '|', '[\"mobile\"]', '|')\n",
      "(55, '|', '{\"mobile\": 0.122}', '|', '[\"mobile\"]', '|')\n",
      "(56, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(57, '|', '{\"mobile\": 0.142}', '|', '[\"mobile\"]', '|')\n",
      "(58, '|', '{}', '|', '[]', '|')\n",
      "(59, '|', '{}', '|', '[]', '|')\n",
      "(60, '|', '{}', '|', '[]', '|')\n",
      "(61, '|', '{}', '|', '[]', '|')\n",
      "(62, '|', '{}', '|', '[]', '|')\n",
      "(63, '|', '{}', '|', '[]', '|')\n",
      "(64, '|', '{\"mobile\": 0.149}', '|', '[\"mobile\"]', '|')\n",
      "(65, '|', '{}', '|', '[]', '|')\n",
      "(66, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(67, '|', '{}', '|', '[]', '|')\n",
      "(68, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(69, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(70, '|', '{}', '|', '[]', '|')\n",
      "(71, '|', '{}', '|', '[]', '|')\n",
      "(72, '|', '{}', '|', '[]', '|')\n",
      "(73, '|', '{}', '|', '[]', '|')\n",
      "(74, '|', '{\"mobile\": 0.133}', '|', '[\"mobile\"]', '|')\n",
      "(75, '|', '{\"mobile\": 0.11}', '|', '[\"mobile\"]', '|')\n",
      "(76, '|', '{\"mobile\": 0.204}', '|', '[\"mobile\"]', '|')\n",
      "(77, '|', '{}', '|', '[]', '|')\n",
      "(78, '|', '{}', '|', '[]', '|')\n",
      "(79, '|', '{}', '|', '[]', '|')\n",
      "(80, '|', '{}', '|', '[]', '|')\n",
      "(81, '|', '{}', '|', '[]', '|')\n",
      "(82, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(83, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(84, '|', '{\"mobile\": 0.176}', '|', '[\"mobile\"]', '|')\n",
      "(85, '|', '{\"mobile\": 0.104}', '|', '[\"mobile\"]', '|')\n",
      "(86, '|', '{\"mobile\": 0.176}', '|', '[\"mobile\"]', '|')\n",
      "(87, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(88, '|', '{\"mobile\": 0.122}', '|', '[\"mobile\"]', '|')\n",
      "(89, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(90, '|', '{}', '|', '[]', '|')\n",
      "(91, '|', '{\"mobile\": 0.11}', '|', '[\"mobile\"]', '|')\n",
      "(92, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(93, '|', '{\"mobile\": 0.176}', '|', '[\"mobile\"]', '|')\n",
      "(94, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(95, '|', '{}', '|', '[]', '|')\n",
      "(96, '|', '{}', '|', '[]', '|')\n",
      "(97, '|', '{}', '|', '[]', '|')\n",
      "(98, '|', '{}', '|', '[]', '|')\n",
      "(99, '|', '{}', '|', '[]', '|')\n",
      "Time taken to predict data cosine :  0.10621714592\n",
      "Updating elasticsearch data with Cosine Distance.......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('elasticsearch data updated in ', 2.6352479457855225, 'sec')\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "('index', '||', 'tagscore', '||', 'top_tag', '||', 'actual_tag')\n",
      "(0, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(1, '|', '{\"mobile\": 0.103}', '|', '[\"mobile\"]', '|')\n",
      "(2, '|', '{}', '|', '[]', '|')\n",
      "(3, '|', '{}', '|', '[]', '|')\n",
      "(4, '|', '{}', '|', '[]', '|')\n",
      "(5, '|', '{}', '|', '[]', '|')\n",
      "(6, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(7, '|', '{}', '|', '[]', '|')\n",
      "(8, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(9, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(10, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(11, '|', '{}', '|', '[]', '|')\n",
      "(12, '|', '{}', '|', '[]', '|')\n",
      "(13, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(14, '|', '{}', '|', '[]', '|')\n",
      "(15, '|', '{}', '|', '[]', '|')\n",
      "(16, '|', '{}', '|', '[]', '|')\n",
      "(17, '|', '{\"mobile\": 0.142}', '|', '[\"mobile\"]', '|')\n",
      "(18, '|', '{\"mobile\": 0.204}', '|', '[\"mobile\"]', '|')\n",
      "(19, '|', '{\"mobile\": 0.102}', '|', '[\"mobile\"]', '|')\n",
      "(20, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(21, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(22, '|', '{}', '|', '[]', '|')\n",
      "(23, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(24, '|', '{}', '|', '[]', '|')\n",
      "(25, '|', '{}', '|', '[]', '|')\n",
      "(26, '|', '{}', '|', '[]', '|')\n",
      "(27, '|', '{}', '|', '[]', '|')\n",
      "(28, '|', '{}', '|', '[]', '|')\n",
      "(29, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(30, '|', '{}', '|', '[]', '|')\n",
      "(31, '|', '{}', '|', '[]', '|')\n",
      "(32, '|', '{}', '|', '[]', '|')\n",
      "(33, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(34, '|', '{}', '|', '[]', '|')\n",
      "(35, '|', '{}', '|', '[]', '|')\n",
      "(36, '|', '{\"mobile\": 0.13}', '|', '[\"mobile\"]', '|')\n",
      "(37, '|', '{}', '|', '[]', '|')\n",
      "(38, '|', '{}', '|', '[]', '|')\n",
      "(39, '|', '{}', '|', '[]', '|')\n",
      "(40, '|', '{}', '|', '[]', '|')\n",
      "(41, '|', '{\"mobile\": 0.136}', '|', '[\"mobile\"]', '|')\n",
      "(42, '|', '{\"mobile\": 0.197}', '|', '[\"mobile\"]', '|')\n",
      "(43, '|', '{\"mobile\": 0.197}', '|', '[\"mobile\"]', '|')\n",
      "(44, '|', '{}', '|', '[]', '|')\n",
      "(45, '|', '{\"mobile\": 0.627}', '|', '[\"mobile\"]', '|')\n",
      "(46, '|', '{}', '|', '[]', '|')\n",
      "(47, '|', '{\"mobile\": 0.139}', '|', '[\"mobile\"]', '|')\n",
      "(48, '|', '{}', '|', '[]', '|')\n",
      "(49, '|', '{\"mobile\": 0.36}', '|', '[\"mobile\"]', '|')\n",
      "(50, '|', '{}', '|', '[]', '|')\n",
      "(51, '|', '{}', '|', '[]', '|')\n",
      "(52, '|', '{}', '|', '[]', '|')\n",
      "(53, '|', '{}', '|', '[]', '|')\n",
      "(54, '|', '{\"mobile\": 0.25}', '|', '[\"mobile\"]', '|')\n",
      "(55, '|', '{}', '|', '[]', '|')\n",
      "(56, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(57, '|', '{}', '|', '[]', '|')\n",
      "(58, '|', '{}', '|', '[]', '|')\n",
      "(59, '|', '{}', '|', '[]', '|')\n",
      "(60, '|', '{}', '|', '[]', '|')\n",
      "(61, '|', '{}', '|', '[]', '|')\n",
      "(62, '|', '{\"mobile\": 0.153}', '|', '[\"mobile\"]', '|')\n",
      "(63, '|', '{}', '|', '[]', '|')\n",
      "(64, '|', '{}', '|', '[]', '|')\n",
      "(65, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(66, '|', '{}', '|', '[]', '|')\n",
      "(67, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "(68, '|', '{}', '|', '[]', '|')\n",
      "(69, '|', '{}', '|', '[]', '|')\n",
      "(70, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(71, '|', '{}', '|', '[]', '|')\n",
      "(72, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(73, '|', '{}', '|', '[]', '|')\n",
      "(74, '|', '{}', '|', '[]', '|')\n",
      "(75, '|', '{\"mobile\": 0.131}', '|', '[\"mobile\"]', '|')\n",
      "(76, '|', '{}', '|', '[]', '|')\n",
      "(77, '|', '{}', '|', '[]', '|')\n",
      "(78, '|', '{}', '|', '[]', '|')\n",
      "(79, '|', '{\"mobile\": 0.145}', '|', '[\"mobile\"]', '|')\n",
      "(80, '|', '{}', '|', '[]', '|')\n",
      "(81, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(82, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(83, '|', '{}', '|', '[]', '|')\n",
      "(84, '|', '{}', '|', '[]', '|')\n",
      "(85, '|', '{}', '|', '[]', '|')\n",
      "(86, '|', '{}', '|', '[]', '|')\n",
      "(87, '|', '{}', '|', '[]', '|')\n",
      "(88, '|', '{}', '|', '[]', '|')\n",
      "(89, '|', '{}', '|', '[]', '|')\n",
      "(90, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(91, '|', '{}', '|', '[]', '|')\n",
      "(92, '|', '{}', '|', '[]', '|')\n",
      "(93, '|', '{}', '|', '[]', '|')\n",
      "(94, '|', '{}', '|', '[]', '|')\n",
      "(95, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(96, '|', '{}', '|', '[]', '|')\n",
      "(97, '|', '{}', '|', '[]', '|')\n",
      "(98, '|', '{}', '|', '[]', '|')\n",
      "(99, '|', '{}', '|', '[]', '|')\n",
      "Time taken to predict data cosine :  0.107771873474\n",
      "Updating elasticsearch data with Cosine Distance.......\n",
      "('elasticsearch data updated in ', 1.951484203338623, 'sec')\n",
      "Document Similarity Analysis using Cosine Similarity\n",
      "('index', '||', 'tagscore', '||', 'top_tag', '||', 'actual_tag')\n",
      "(0, '|', '{\"mobile\": 0.133}', '|', '[\"mobile\"]', '|')\n",
      "(1, '|', '{\"mobile\": 1.0}', '|', '[\"mobile\"]', '|')\n",
      "(2, '|', '{}', '|', '[]', '|')\n",
      "(3, '|', '{\"mobile\": 0.102}', '|', '[\"mobile\"]', '|')\n",
      "(4, '|', '{}', '|', '[]', '|')\n",
      "(5, '|', '{}', '|', '[]', '|')\n",
      "(6, '|', '{}', '|', '[]', '|')\n",
      "(7, '|', '{}', '|', '[]', '|')\n",
      "(8, '|', '{\"mobile\": 0.104}', '|', '[\"mobile\"]', '|')\n",
      "(9, '|', '{}', '|', '[]', '|')\n",
      "(10, '|', '{}', '|', '[]', '|')\n",
      "(11, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(12, '|', '{\"mobile\": 0.13}', '|', '[\"mobile\"]', '|')\n",
      "(13, '|', '{}', '|', '[]', '|')\n",
      "(14, '|', '{}', '|', '[]', '|')\n",
      "(15, '|', '{\"mobile\": 0.134}', '|', '[\"mobile\"]', '|')\n",
      "(16, '|', '{\"mobile\": 0.25}', '|', '[\"mobile\"]', '|')\n",
      "(17, '|', '{}', '|', '[]', '|')\n",
      "(18, '|', '{\"mobile\": 0.128}', '|', '[\"mobile\"]', '|')\n",
      "(19, '|', '{}', '|', '[]', '|')\n",
      "(20, '|', '{}', '|', '[]', '|')\n",
      "(21, '|', '{}', '|', '[]', '|')\n",
      "(22, '|', '{}', '|', '[]', '|')\n",
      "(23, '|', '{}', '|', '[]', '|')\n",
      "(24, '|', '{\"mobile\": 0.153}', '|', '[\"mobile\"]', '|')\n",
      "(25, '|', '{}', '|', '[]', '|')\n",
      "(26, '|', '{}', '|', '[]', '|')\n",
      "(27, '|', '{\"mobile\": 0.12}', '|', '[\"mobile\"]', '|')\n",
      "(28, '|', '{}', '|', '[]', '|')\n",
      "(29, '|', '{\"mobile\": 0.125}', '|', '[\"mobile\"]', '|')\n",
      "Time taken to predict data cosine :  0.0314080715179\n",
      "Updating elasticsearch data with Cosine Distance.......\n",
      "('elasticsearch data updated in ', 1.4136860370635986, 'sec')\n",
      "Time taken to read 330 train document is :  310.150779963\n"
     ]
    }
   ],
   "source": [
    "def read_elasticsearch_data_test(from_i, to_i, field):\n",
    "    body = {\n",
    "                \"from\": from_i,\n",
    "                \"size\": to_i,\n",
    "                \"query\": {\n",
    "                    \"exists\": {\n",
    "                        \"field\": field\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    s = Search.from_dict(body)\n",
    "    s = s.index(index)\n",
    "    s = s.doc_type(doc_type)\n",
    "    s.using(client)\n",
    "    return s.execute()\n",
    "\n",
    "start_test_read  = time.time()\n",
    "\n",
    "print 'Reading test documents.........'\n",
    "raw_text =[]\n",
    "\n",
    "counter = 0\n",
    "while test_no > counter * single_time_execute_no_documents:\n",
    "    raw_text = read_elasticsearch_data_test(counter * single_time_execute_no_documents, single_time_execute_no_documents, \"pathforsearch\")\n",
    "    if len(raw_text) > 0:\n",
    "        json_update = run_classification(raw_text)\n",
    "        response = elasticsearch_bulk_update(json_update)\n",
    "        if response is None :\n",
    "            print(\"Error in uploading to elasticsearch\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "\n",
    "print 'Time taken to read',  test_no ,'train document is : ', time.time() - start_test_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def elasticsearch_bulk_update(tags_scores) :\n",
    "    start_update  = time.time()\n",
    "    try:\n",
    "        print('Updating elasticsearch data with Cosine Distance.......')\n",
    "        response = requests.post(elastic_url, data=''.join(tags_scores))\n",
    "        print('elasticsearch data updated in ', time.time() - start_update, 'sec')\n",
    "    except Exception as e:\n",
    "        print(\"Error in bulk update to server %s : \"%elastic_url, e) \n",
    "        response = None\n",
    "    return response"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def run_classification(raw_text):\n",
    "    texts = []\n",
    "    ids = []\n",
    "    for row in raw_text:\n",
    "        texts.append(row['preprocessed_text'])\n",
    "        ids.append(row['file_id'])\n",
    "        \n",
    "    query_docs_tfidf = vectorizer.transform(texts)\n",
    "    print 'Document Similarity Analysis using Cosine Similarity'\n",
    "    print('index', '||', 'tagscore', '||', 'top_tag' , '||', 'actual_tag')\n",
    "    start_cosine = time.time()\n",
    "    tag_pred_cos = []\n",
    "    json_update = []\n",
    "    for index, doc_id in enumerate(ids):\n",
    "\n",
    "        doc_tfidf = query_docs_tfidf[index]\n",
    "        top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                                 tfidf_features,\n",
    "                                                 top_n=top_n_tags)\n",
    "        top5 = []\n",
    "        sim_scores = []\n",
    "        for doc_index, sim_score in top_similar_docs:\n",
    "            if sim_score > threshold_cosine :\n",
    "                top5.append(TagScore(labels[doc_index], sim_score))\n",
    "\n",
    "\n",
    "        tagsscore_dir = get_top_tag(top5)\n",
    "\n",
    "        tag_pred_cos.append(tagsscore_dir)\n",
    "        key = ids[index]\n",
    "        tags = str(tagsscore_dir.keys()).replace('\\'', '\"')\n",
    "        tagscore = str(tagsscore_dir).replace('\\'', '\"')\n",
    "        print(index, '|', tagscore, '|', tags, '|')\n",
    "        json_update.append('''\n",
    "        { \\\"update\\\": { \\\"_index\\\": \\\"ccthinclient\\\", \\\"_type\\\": \\\"thin_client\\\", \\\"_id\\\": \\\"%s\\\", \\\"_retry_on_conflict\\\" : 1} }\n",
    "        { \\\"script\\\":{  \\\"inline\\\": \\\"ctx._source.tagscore=params.tagscore;ctx._source.tags=params.tags\\\", \\\"params\\\":{ \\\"tagscore\\\":%s, \\\"tags\\\":%s }   } }'''%(key, tagscore, tags))\n",
    "\n",
    "    json_update.append('''\\n''')    \n",
    "    \n",
    "    print 'Time taken to predict data cosine : ', time.time() - start_cosine\n",
    "    return json_update\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class TagScore:\n",
    "    def __init__(self, tag, sim_score):\n",
    "        self.tag = tag\n",
    "        self.sim_score = sim_score\n",
    "\n",
    "\n",
    "def get_top_tag(top5):\n",
    "    toptag_dir = {}\n",
    "    if len(top5) > 0:\n",
    "        for row in top5:\n",
    "            if isinstance(row, TagScore):\n",
    "                for tag in row.tag:\n",
    "                    if(tag in toptag_dir and \n",
    "                       toptag_dir[str(tag)] > row.sim_score):\n",
    "                        pass\n",
    "                    else :\n",
    "                        toptag_dir[str(tag)] = row.sim_score\n",
    "    return toptag_dir\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class Text_Document:\n",
    "    def __init__(self, _id, _text):\n",
    "        self.doc_id = _id\n",
    "        self.text = _text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class TagScore:\n",
    "    def __init__(self, tag, sim_score):\n",
    "        self.tag = tag\n",
    "        self.sim_score = sim_score\n",
    "\n",
    "\n",
    "def get_top_tag(top5):\n",
    "    toptag_dir = {}\n",
    "    if len(top5) > 0:\n",
    "        for row in top5:\n",
    "            if isinstance(row, TagScore):\n",
    "                for tag in row.tag:\n",
    "                    if(tag in toptag_dir and \n",
    "                       toptag_dir[str(tag)] > row.sim_score):\n",
    "                        pass\n",
    "                    else :\n",
    "                        toptag_dir[str(tag)] = row.sim_score\n",
    "    return toptag_dir\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def calculate_similarity(docs, query_docs_tfidf, tfidf_features)\n",
    "    print 'Document Similarity Analysis using Cosine Similarity'\n",
    "    print('index', '||', 'tagscore', '||', 'top_tag' , '||', 'actual_tag')\n",
    "    start_cosine = time.time()\n",
    "    tag_pred_cos = []\n",
    "    json_update = []\n",
    "    for index, doc_id in enumerate(docs.doc_id):\n",
    "\n",
    "        doc_tfidf = query_docs_tfidf[index]\n",
    "        top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                                 tfidf_features,\n",
    "                                                 top_n=top_n_tags)\n",
    "        top5 = []\n",
    "        sim_scores = []\n",
    "        for doc_index, sim_score in top_similar_docs:\n",
    "            if sim_score > threshold_cosine :\n",
    "                top5.append(TagScore(train.tags[doc_index], sim_score))\n",
    "\n",
    "\n",
    "        tagsscore_dir = get_top_tag(top5)\n",
    "\n",
    "        tag_pred_cos.append(tagsscore_dir)\n",
    "        key = test['index'][index]\n",
    "        tags = str(tagsscore_dir.keys()).replace('\\'', '\"')\n",
    "        tagscore = str(tagsscore_dir).replace('\\'', '\"')\n",
    "        print(index, '|', tagscore, '|', tags, '|', test.name[index])\n",
    "        json_update.append('''\n",
    "        { \\\"update\\\": { \\\"_index\\\": \\\"ccthinclient\\\", \\\"_type\\\": \\\"thin_client\\\", \\\"_id\\\": \\\"%s\\\", \\\"_retry_on_conflict\\\" : 1} }\n",
    "        { \\\"script\\\":{  \\\"inline\\\": \\\"ctx._source.tagscore=params.tagscore;ctx._source.tags=params.tags\\\", \\\"params\\\":{ \\\"tagscore\\\":%s, \\\"tags\\\":%s }   } }'''%(key, tagscore, tags))\n",
    "\n",
    "    json_update.append('''\\n''')    \n",
    "    print 'Time taken to predict data cosine : ', time.time() - start_cosine\n",
    "    return json_update"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def elasticsearch_bulk_update(docs, tfidf_vectorizer, tfidf_features) :\n",
    "    start_update  = time.time()\n",
    "    tags_scores = calculate_similarity(docs, tfidf_vectorizer, tfidf_features)\n",
    "    try:\n",
    "        print('Updating elasticsearch data with Cosine Distance.......')\n",
    "        response = requests.post(elastic_url, data=''.join(tags_scores))\n",
    "        print('elasticsearch data updated in ', time.time() - start_update, 'sec')\n",
    "    except Exception as e:\n",
    "        print(\"Error in bulk update to server %s : \"%elastic_url, e) \n",
    "        response = None\n",
    "    return response"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def run_classification(docs, tfidf_vectorizer, tfidf_features):\n",
    "    \n",
    "    norm_query_docs = [doc.text for doc in docs]\n",
    "    \n",
    "    try :\n",
    "        query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)\n",
    "    except Exception as e:\n",
    "        print(\"Can not transform test documents to vector\", e)\n",
    "        return False\n",
    "    \n",
    "    if elasticsearch_bulk_update(docs, tfidf_vectorizer, tfidf_features) is None:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%%time\n",
    "threadErrors = [] #global list\n",
    "try :\n",
    "    no_docs_in_a_thread = 100\n",
    "    counter = 0\n",
    "    docs_in_a_thread = []\n",
    "    for res in scroll:\n",
    "        while True:\n",
    "            try:\n",
    "                if counter < no_docs_in_a_thread:\n",
    "\n",
    "                    if 'preprocessed_text' in res['_source'] and \\\n",
    "                    len(res['_source']['preprocessed_text']) > 0 and \\\n",
    "                    'file_id' in res['_source']:\n",
    "\n",
    "                        docs_in_a_thread.append(Text_Document(res['_source']['file_id'], res['_source']['preprocessed_text']))\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                else :\n",
    "                    try :\n",
    "                        print('Preprocessing thread')\n",
    "                        try:\n",
    "                            with open (model_file_name, 'rb') as fp:\n",
    "                                tfidf_vectorizer = pickle.load(fp)\n",
    "                        except IOError as e:\n",
    "                            print(\"Can not read %s file\"%model_file_name, e)\n",
    "                            return \n",
    "\n",
    "                        try:\n",
    "                            with open (features_file_name, 'rb') as fp:\n",
    "                                tfidf_features = pickle.load(fp)\n",
    "                        except IOError as e:\n",
    "                            print(\"Can not read %s file\"%features_file_name, e)\n",
    "                            return \n",
    "                        traversing = Thread(target=run_classification, args=(docs_in_a_thread, tfidf_vectorizer, tfidf_features,))\n",
    "                        traversing.start()\n",
    "                    except ThreadError as te:\n",
    "                        threadErrors.append([repr(te), current_thread.name]) \n",
    "                    #run_preprocessing(docs_in_a_thread)\n",
    "                    docs_in_a_thread = []\n",
    "                    counter = 0\n",
    "            except Exception as e :\n",
    "                try:\n",
    "                    res = scroll.next()\n",
    "                except StopIteration as e:\n",
    "                    print(\"Document is skiped while reading\", e)\n",
    "            break\n",
    "    if counter is not 0:\n",
    "        try :\n",
    "            traversing = Thread(target=run_classification, args=(docs_in_a_thread,))\n",
    "            traversing.start()\n",
    "        except ThreadError as te:\n",
    "            threadErrors.append([repr(te), current_thread.name]) \n",
    "        \n",
    "    if len(threadErrors) > 0: #check if there are any errors \n",
    "        for e in threadErrors:\n",
    "            print(threadErrors[e][0]+' occurred in thread: '+threadErrors[e][1])\n",
    "\n",
    "except ElasticsearchException as es:\n",
    "    print(\"Error in scrolling documents from server\", es)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
