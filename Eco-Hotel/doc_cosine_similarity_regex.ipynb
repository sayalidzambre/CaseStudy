{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#from normalization import normalize_corpus,review_to_sentences\n",
    "from utils import build_feature_matrix\n",
    "from ast import literal_eval\n",
    "\n",
    "sys.path.append(\"/usr/lib/python2.7/dist-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################################\n",
      "Execution Started............\n"
     ]
    }
   ],
   "source": [
    "start_script = time.time()\n",
    "print ''\n",
    "print \"#######################################################################################\"\n",
    "print \"Execution Started............\"\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta  = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import DocType, String, Date, Integer\n",
    "from elasticsearch_dsl.connections import connections\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q\n",
    "import requests\n",
    "\n",
    "index = 'ccthinclient'\n",
    "doc_type = 'thin_client'\n",
    "\n",
    "elasticsearch_host = '35.167.139.79:9200'\n",
    "elastic_url = 'http://35.167.139.79:9200/_bulk?pretty=true'\n",
    "\n",
    "#elasticsearch_host = '52.24.198.221:8080'\n",
    "#elastic_url = 'http://52.24.198.221:8080/_bulk?pretty=true'\n",
    "\n",
    "\n",
    "# Define a default Elasticsearch client\n",
    "client = connections.create_connection(hosts=[elasticsearch_host])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train documents :  4\n",
      "Total test documents :  330\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@timeit\n",
    "def get_no_of_docs_from_elasticsearch(field):\n",
    "    body = {}\n",
    "    if field is 'tags':\n",
    "        body = {\n",
    "                \"query\": {\n",
    "                    \"match\":{\n",
    "                            \"taggedbyadmin\":\"true\"\n",
    "                    }\n",
    "                }\n",
    "        }\n",
    "    else:\n",
    "         body = {\n",
    "            \"query\": {\n",
    "                \"exists\": {\n",
    "                    \"field\": field\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    s = Search.from_dict(body)\n",
    "    s = s.index(index)\n",
    "    s = s.doc_type(doc_type)\n",
    "    s.using(client)\n",
    "    return s.count()\n",
    "\n",
    "train_no,secs = get_no_of_docs_from_elasticsearch('tags')\n",
    "test_no,secs = get_no_of_docs_from_elasticsearch('pathforsearch')\n",
    "print \"Total train documents : \", train_no\n",
    "print \"Total test documents : \", test_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@timeit\n",
    "def read_elasticsearch_data(to_i):\n",
    "    body = {\n",
    "                \"from\": 0,\n",
    "                \"size\": to_i,\n",
    "                \"query\": {\n",
    "                    \"match\":{\n",
    "                        \"taggedbyadmin\":\"true\"\n",
    "                      }\n",
    "                    \n",
    "                }\n",
    "            }\n",
    "    s = Search.from_dict(body)\n",
    "    s = s.index(index)\n",
    "    s = s.doc_type(doc_type)\n",
    "    s.using(client)\n",
    "    return s.execute()\n",
    "print 'Reading train documents.........'\n",
    "raw_train, secs = read_elasticsearch_data(train_no)\n",
    "print 'Time taken to read',  train_no ,'train document is : ', secs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_elasticsearch_data_test(from_i, to_i, field):\n",
    "    body = {\n",
    "                \"from\": from_i,\n",
    "                \"size\": to_i,\n",
    "                \"query\": {\n",
    "                    \"exists\": {\n",
    "                        \"field\": field\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    s = Search.from_dict(body)\n",
    "    s = s.index(index)\n",
    "    s = s.doc_type(doc_type)\n",
    "    s.using(client)\n",
    "    return s.execute()\n",
    "\n",
    "start_test_read  = time.time()\n",
    "\n",
    "print 'Reading test documents.........'\n",
    "raw_test =[]\n",
    "single_time_read_count = 100\n",
    "counter = 0\n",
    "while test_no > counter * single_time_read_count:\n",
    "    raw_test.extend(read_elasticsearch_data_test(counter * single_time_read_count, single_time_read_count, \"pathforsearch\"))\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "print 'Time taken to read',  test_no ,'train document is : ', time.time() - start_test_read"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "with open('raw_test.data', 'wb') as fp:\n",
    "    pickle.dump(raw_test, fp)\n",
    "    \n",
    "'''\n",
    "with open ('raw_test.data', 'rb') as fp:\n",
    "    raw_test = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def createDataFrame(rawdata, data_use):\n",
    "    fileid = []\n",
    "    text = []\n",
    "    tags = []\n",
    "    names = []\n",
    "    for i in range(len(rawdata)):\n",
    "        if('file_text' in rawdata[i] and\n",
    "           'name' in rawdata[i] and\n",
    "           len(rawdata[i].file_text.strip()) > 0 and\n",
    "              'file_id' in rawdata[i]):\n",
    "            \n",
    "            if ('train' in data_use and 'taggedbyadmin' in rawdata[i] and\n",
    "                len(rawdata[i].tags) > 0):\n",
    "                text.append(rawdata[i].file_text)\n",
    "                fileid.append(rawdata[i].file_id)\n",
    "                tags.append(rawdata[i].tags)\n",
    "                names.append(rawdata[i].name)\n",
    "            elif('test' in data_use and 'taggedbyadmin' not in rawdata[i]):\n",
    "                text.append(rawdata[i].file_text)\n",
    "                fileid.append(rawdata[i].file_id)\n",
    "                tags.append([])\n",
    "                names.append(rawdata[i].name)\n",
    "    return pd.DataFrame(\n",
    "        {'index': fileid,\n",
    "         'text': text,\n",
    "         'tags': tags,\n",
    "         'name': names\n",
    "        })\n",
    "     \n",
    "\n",
    "train = createDataFrame(raw_train, 'train')\n",
    "print 'size of train documents %d rows and %d columns' %train.shape\n",
    "test = createDataFrame(raw_test, 'test')\n",
    "print 'size of test documents %d rows and %d columns' %test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from HTMLParser import HTMLParser\n",
    "import unicodedata\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "html_parser = HTMLParser()\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list = stopword_list + ['mr', 'mrs', 'come', 'go', 'get',\n",
    "                                 'tell', 'listen', 'one', 'two', 'three',\n",
    "                                 'four', 'five', 'six', 'seven', 'eight',\n",
    "                                 'nine', 'zero', 'join', 'find', 'make',\n",
    "                                 'say', 'ask', 'tell', 'see', 'try', 'back',\n",
    "                                 'also', '', None, ' ']\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "    \n",
    "\n",
    "def word_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = text\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n",
    "    return filtered_tokens\n",
    "    \n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    tokens = text\n",
    "    filtered_tokens = [token for token in tokens if token.strip() not in stopword_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(words):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    def pos_tagging(words):\n",
    "        return nltk.pos_tag(words)\n",
    "    \n",
    "    tagged_words = pos_tagging(words)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_words]\n",
    "    return tagged_lower_text\n",
    "    \n",
    "\n",
    "\n",
    "def lemmatize_text(words):\n",
    "    pos_tagged_words = pos_tag_text(words)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_words]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def parse_hippa(text):\n",
    "    hippa_pattern = '[[^a-zA-Z0-9]*/*[[^a-zA-Z0-9., -_]*/*[^a-zA-Z0-9., -_]*]*~]*'    \n",
    "    if re.match(hippa_pattern, text)    :\n",
    "        text = re.sub('\\*','\\* ', text)\n",
    "        text = re.sub('~',' ~ ', text)\n",
    "        return True, text\n",
    " \n",
    "    return False, text \n",
    "\n",
    "\n",
    "\n",
    "def normalize_corpus(line, lemmatize=True):\n",
    "    \n",
    "    line = html_parser.unescape(line)\n",
    "    line = expand_contractions(line, CONTRACTION_MAP)\n",
    "    \n",
    "    ishippa, line = parse_hippa(line)\n",
    "    \n",
    "    words = word_tokenize(line)\n",
    "    \n",
    "    if not ishippa:\n",
    "        words = remove_special_characters(words)\n",
    "    \n",
    "    words = remove_stopwords(words)\n",
    "    \n",
    "    if lemmatize:\n",
    "        words = lemmatize_text(words)\n",
    "    return ' '.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'train data preprocessing started...........'\n",
    "start_train_preprocess  = time.time()\n",
    "norm_corpus = train.text.apply(normalize_corpus)\n",
    "print 'Model data preprocessing is finished in : ', time.time() - start_train_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_model = time.time()\n",
    "print 'model building started...........'\n",
    "\n",
    "smooth_idf = True\n",
    "use_idf = True\n",
    "min_df = 0.1\n",
    "max_df = 0.8\n",
    "sublinear_tf = True\n",
    "threshold_cosine =0.3\n",
    "\n",
    "if len(norm_corpus) == 1:\n",
    "    smooth_idf = True\n",
    "    use_idf = True\n",
    "    min_df = 1\n",
    "    max_df = 1.0\n",
    "    sublinear_tf=True\n",
    "    threshold_cosine=0.4\n",
    "\n",
    "tfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus,\n",
    "                                                feature_type='tfidf',\n",
    "                                                ngram_range=(1, 3), \n",
    "                                                min_df=min_df, max_df=max_df, sublinear_tf=sublinear_tf,\n",
    "                                                use_idf=use_idf, smooth_idf=smooth_idf)\n",
    "print('Model building finished in : ', time.time() - start_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_test_preprocess  = time.time()\n",
    "# normalize and extract features from the query corpus\n",
    "print 'Test data preprocessing started.........'\n",
    "norm_query_docs =  test.text.apply(normalize_corpus) #(test.text, lemmatize=True)\n",
    "#norm_query_docs = [' '.join(doc) for doc in norm_query_docs]\n",
    "print 'Test data preprocessing is finished in : ', time.time() - start_test_preprocess"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "with open('outfile_mobile', 'wb') as fp:\n",
    "    pickle.dump(norm_query_docs, fp)\n",
    "'''\n",
    "with open ('outfile_mobile', 'rb') as fp:\n",
    "    norm_query_docs = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_vectorise  = time.time()\n",
    "print 'Test data vectorzing started.......'\n",
    "query_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)\n",
    "print 'Test data vectorizing is finished in : ', time.time() - start_vectorise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class TagScore:\n",
    "    def __init__(self, tag, sim_score):\n",
    "        self.tag = tag\n",
    "        self.sim_score = sim_score\n",
    "\n",
    "\n",
    "def get_top_tag(top5):\n",
    "    toptag_dir = {}\n",
    "    if len(top5) > 0:\n",
    "        for row in top5:\n",
    "            if isinstance(row, TagScore):\n",
    "                for tag in row.tag:\n",
    "                    if(tag in toptag_dir and \n",
    "                       toptag_dir[str(tag)] > row.sim_score):\n",
    "                        pass\n",
    "                    else :\n",
    "                        toptag_dir[str(tag)] = row.sim_score\n",
    "    return toptag_dir\n",
    "\n",
    "\n",
    "def compute_cosine_similarity(doc_features, corpus_features,\n",
    "                              top_n=3):\n",
    "    # get document vectors\n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute similarities\n",
    "    similarity = np.dot(doc_features, \n",
    "                        corpus_features.T)\n",
    "    # get docs with highest similarity scores\n",
    "    top_docs = similarity.argsort()[::-1][:top_n]\n",
    "    top_docs_with_score = [(index, round(similarity[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score\n",
    "\n",
    "\n",
    "print 'Document Similarity Analysis using Cosine Similarity'\n",
    "print('index', '||', 'tagscore', '||', 'top_tag' , '||', 'actual_tag')\n",
    "start_cosine = time.time()\n",
    "tag_pred_cos = []\n",
    "json_update = []\n",
    "for index, doc in enumerate(test.text):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_cosine_similarity(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n=3)\n",
    "    top5 = []\n",
    "    sim_scores = []\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        if sim_score > threshold_cosine :\n",
    "            top5.append(TagScore(train.tags[doc_index], sim_score))\n",
    "            \n",
    "\n",
    "    tagsscore_dir = get_top_tag(top5)\n",
    "    \n",
    "    tag_pred_cos.append(tagsscore_dir)\n",
    "    key = test['index'][index]\n",
    "    tags = str(tagsscore_dir.keys()).replace('\\'', '\"')\n",
    "    tagscore = str(tagsscore_dir).replace('\\'', '\"')\n",
    "    print(index, '|', tagscore, '|', tags, '|', test.name[index])\n",
    "    json_update.append('''\n",
    "    { \\\"update\\\": { \\\"_index\\\": \\\"ccthinclient\\\", \\\"_type\\\": \\\"thin_client\\\", \\\"_id\\\": \\\"%s\\\", \\\"_retry_on_conflict\\\" : 1} }\n",
    "    { \\\"script\\\":{  \\\"inline\\\": \\\"ctx._source.tagscore=params.tagscore;ctx._source.tags=params.tags\\\", \\\"params\\\":{ \\\"tagscore\\\":%s, \\\"tags\\\":%s }   } }'''%(key, tagscore, tags))\n",
    "\n",
    "json_update.append('''\\n''')    \n",
    "print 'Time taken to predict data cosine : ', time.time() - start_cosine\n",
    "test['pred_tags_cosine'] = tag_pred_cos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train[0].file_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hellinger Bhattacharya Distance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def compute_hellinger_bhattacharya_distance(doc_features, corpus_features,\n",
    "                                            top_n=3):\n",
    "    # get document vectors                                            \n",
    "    doc_features = doc_features.toarray()[0]\n",
    "    corpus_features = corpus_features.toarray()\n",
    "    # compute hb distances\n",
    "    distance = np.hstack(\n",
    "                    np.sqrt(0.5 *\n",
    "                            np.sum(\n",
    "                                np.square(np.sqrt(doc_features) - \n",
    "                                          np.sqrt(corpus_features)), \n",
    "                                axis=1)))\n",
    "    # get docs with lowest distance scores                            \n",
    "    top_docs = distance.argsort()[:top_n]\n",
    "    top_docs_with_score = [(index, round(distance[index], 3))\n",
    "                            for index in top_docs]\n",
    "    return top_docs_with_score \n",
    "\n",
    "print 'Document Similarity Analysis using Hellinger-Bhattacharya distance'\n",
    "start_h_b = time.time()\n",
    "tag_pred_h_b = []\n",
    "for index, doc in enumerate(test.text):\n",
    "    \n",
    "    doc_tfidf = query_docs_tfidf[index]\n",
    "    top_similar_docs = compute_hellinger_bhattacharya_distance(doc_tfidf,\n",
    "                                             tfidf_features,\n",
    "                                             top_n=5)\n",
    "    top5 = []\n",
    "    for doc_index, sim_score in top_similar_docs:\n",
    "        if sim_score > 6 :\n",
    "            top5.append(\"\".join(train.tags[doc_index]))\n",
    "    \n",
    "    top_tags = get_top_tag(top5)\n",
    "    print(index, top5 , top_similar_docs, \"$$$$$\", top_tags , '@@@@@@@@', test.name[index])\n",
    "    tag_pred_h_b.append(top_tags)\n",
    "\n",
    "print('Time taken to predict Hellinger Bhattacharya Distance :', time.time() - start_h_b )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update ElasticSearch with predicted tag using cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elasticsearch_bulk_update(to_update) :\n",
    "    start_update  = time.time()\n",
    "    print 'Updating elasticsearch data with Cosine Distance.......'\n",
    "    response = requests.post(elastic_url, data=''.join(to_update))\n",
    "    print'elasticsearch data updated in ', time.time() - start_update, 'sec'\n",
    "\n",
    "elasticsearch_bulk_update(json_update) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ''\n",
    "print 'Total time for one execution is: ' , time.time()-start_script\n",
    "print ''\n",
    "#except Exception as e:\n",
    "#        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While converting to .py delete below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_update[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in raw_train:\n",
    "    print row.file_id, row.tags, row.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV of predicted tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "test['pred_tags_h_b'] = tag_pred_h_b\n",
    "preddf = test[['index', 'pred_tags_cosine', 'pred_tags_h_b']]\n",
    "preddf.to_csv('predicted_tags.csv')\n",
    "print('tags are predicted in predicted_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Update Elastic Search Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://35.167.139.79:9200/_bulk?pretty=true'\n",
    "data = \"\"\"{ \"update\": { \"_index\": \"ccthinclient\", \"_type\": \"thin_client\", \"_id\": \"d86cb9d277a3f6ae32dcb21251ee15ce0705a955\", \"_retry_on_conflict\" : 1} }\n",
    "{ \"doc\" : {\"tags\" : [\"tagsss\"]}} \n",
    "{ \"update\": { \"_index\": \"ccthinclient\", \"_type\": \"thin_client\", \"_id\": \"d86cb9d277a3f6ae32dcb21251ee15ce0705a955\", \"_retry_on_conflict\" : 1} }\n",
    "{ \"doc\" : {\"tags\" : [\"tagsss\"]}} \"\"\"\n",
    "\n",
    "response = requests.post(url, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update actual Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update tags to default value tagsss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://35.167.139.79:9200/_bulk?pretty=true'\n",
    "json_update = ''\n",
    "data = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    key = test['index'][i]\n",
    "\n",
    "    tagsscore_dir = {\"a\": 0.5}\n",
    "    tagscore = str(tagsscore_dir).replace('\\'', '\"')\n",
    "    tag = str(tagsscore_dir.keys()).replace('\\'', '\"')\n",
    "    data.append('''\n",
    "    { \\\"update\\\": { \\\"_index\\\": \\\"ccthinclient\\\", \\\"_type\\\": \\\"thin_client\\\", \\\"_id\\\": \\\"%s\\\", \\\"_retry_on_conflict\\\" : 1} }\n",
    "    { \\\"script\\\":{  \\\"inline\\\": \\\"ctx._source.tagscore=params.tagscore;ctx._source.tags=params.tags\\\", \\\"params\\\":{ \\\"tagscore\\\":%s, \\\"tags\\\":%s }   } }'''%(key,tagscore,tag))\n",
    "  \n",
    "    data.append('''\\n''')\n",
    "response = requests.post(url, data=''.join(data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    key = test['index'][i]\n",
    "    if \"5c91404b59c9d4a093ab830e78dbcf566c547563\" in key:\n",
    "        print True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all documents and check tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = connections.create_connection(hosts=[elasticsearch_host])\n",
    "raw_test = read_elasticsearch_data_test(0, 20, \"pathforsearch\")\n",
    "for i in range(len(raw_test)):\n",
    "    if('file_text' in raw_test[i] and 'name' in raw_test[i] and len(raw_test[i].file_text.strip()) > 0 and \n",
    "       'file_id' in raw_test[i]):\n",
    "            \n",
    "        print(str(raw_test[i].file_id) + \"::::::\" + str(raw_test[i].tagscore) + raw_test[i].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([x for x in tag_pred_cos if 'mobile' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "count = 0\n",
    "for nm in test.name:\n",
    "    if re.search('mc[a-z0-9.]*', nm):\n",
    "        count = count +1\n",
    "    if re.search('nc[a-z0-9.]*', nm):\n",
    "        count = count +1\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -XPOST 'localhost:9200/_bulk?pretty' -H 'Content-Type: application/json' -d'\n",
    "{ \"update\": { \"_index\": \"ccthinclient\", \"_type\": \"thin_client\", \"_id\": \"a7e61fe696ad59cd43cbac6a4a8fa8e1313e0698\", \"_retry_on_conflict\" : 1} }\n",
    "{ \"doc\" : {\"tags\" : [\"test\"]}} \n",
    "{ \"update\": { \"_index\": \"ccthinclient\", \"_type\": \"thin_client\", \"_id\": \"58a5749e362a476fe18812038951a3c696c576cf\", \"_retry_on_conflict\" : 1} }\n",
    "{ \"doc\" : {\"tags\" : [\"test1\"]}} \n",
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://35.167.139.79:9200/_bulk?pretty=true'\n",
    "json_update = ''\n",
    "data = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    key = test['index'][i]\n",
    "\n",
    "    tagsscore_dir = {\"a\": 0.5}\n",
    "    tagscore = str(tagsscore_dir).replace('\\'', '\"')\n",
    "    tag = str(tagsscore_dir.keys()).replace('\\'', '\"')\n",
    "    data.append('''\n",
    "    { \\\"update\\\": { \\\"_index\\\": \\\"ccthinclient\\\", \\\"_type\\\": \\\"thin_client\\\", \\\"_id\\\": \\\"%s\\\", \\\"_retry_on_conflict\\\" : 1} }\n",
    "    { \\\"script\\\":{  \\\"inline\\\": \\\"ctx._source.tagscore=params.tagscore;ctx._source.tags=params.tags\\\", \\\"params\\\":{ \\\"tagscore\\\":%s, \\\"tags\\\":%s }   } }'''%(key,tagscore,tag))\n",
    "  \n",
    "    data.append('''\\n''')\n",
    "response = requests.post(url, data=''.join(data))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -XGET \"http://35.167.139.79:9200/_analyze?tokenizer=standard&char_filters=html_strip&analyzer=english\" -d'\n",
    "{\n",
    "  \"text\" : \"ISA* 00*           * 00*           * ZZ* EMEDNYBAT      * ZZ* ETIN           * 100101* 1000* ^* 00501* 006000600* 0* T* :~GS* HP* EMEDNYBAT* ETIN* 20100101* 1050* 6000600* X* 005010X221A1~ST* 835* 1740~BPR* H* 0* C* NON* * * * * * * * * * * * 20100101~TRN* 1* 10100000000* 1000000000~REF* EV* ETIN~DTM* 405* 20100101~N1* PR* NYSDOH~N3* OFFICE OF HEALTH INSURANCE PROGRAMS* CORNING TOWER, EMPIRE STATE PLAZA~N4* ALBANY* NY* 122370080~PER* BL* PROVIDER SERVICES* TE* 8003439000* UR* www.emedny.org~N1* PE* MAJOR MEDICAL PROVIDER* XX* 9999999995~REF* TJ* 000000000~LX* 1~CLP* PATIENT ACCOUNT NUMBER* 2* 34* 0* * MC* 1000220000000030* 11~NM1* QC* 1* SUBMITTED LAST* SUBMITTED FIRST* * * * MI* LL88888L~NM1* 74* 1* CORRECTED LAST* CORRECTED FIRST~REF* EA* PATIENT ACCOUNT NUMBER~DTM* 232* 20100101~DTM* 233* 20100101~SVC* HC:V2020* 12* 0* * 0~DTM* 472* 20100101~CAS* CO* 29* 12~SVC* HC:V2103* 22* 0* * 0~DTM* 472* 20100101~CAS* CO* 29* 22~SE* 25* 1740~GE* 1* 6000600~IEA* 1* 006000600~\"\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -XGET \"http://35.167.139.79:9200/_analyze?analyzer=english&tokenizer=standard\" -d'\n",
    "{\n",
    "  \"text\" : \"This is a test, which you have to pass man.\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml = '''<web-app xmlns=\"http://java.sun.com/xml/ns/j2ee\"\n",
    "    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "    xsi:schemaLocation=\"http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd\"\n",
    "    version=\"2.4\">\n",
    "\n",
    "    <display-name>HelloWorld Application</display-name>\n",
    "    <description>\n",
    "        This is a simple web application with a source code organization\n",
    "        based on the recommendations of the Application Developer's Guide.\n",
    "    </description>\n",
    "\n",
    "    <servlet>\n",
    "        <servlet-name>HelloServlet</servlet-name>\n",
    "        <servlet-class>examples.Hello</servlet-class>\n",
    "    </servlet>\n",
    "\n",
    "    <servlet-mapping>\n",
    "        <servlet-name>HelloServlet</servlet-name>\n",
    "        <url-pattern>/hello</url-pattern>\n",
    "    </servlet-mapping>\n",
    "\n",
    "</web-app>   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = '''{\n",
    "  \"tokenizer\": \"<([^<>]+)>([^<>]*)<(\\/[^<>]+)>\",\n",
    "  \"text\": %s\n",
    "}'''%(xml)\n",
    "'''\n",
    "curl -XPOST 'http://35.167.139.79:9200/_analyze?pretty' -H 'Content-Type: application/json' -d'\n",
    "{\n",
    "  \"tokenizer\": \"<([^<>]+)>([^<>]*)<(\\/[^<>]+)>\",\n",
    "  \"text\": \"<servlet-mapping> <servlet-name>HelloServlet</servlet-name> <url-pattern>/hello</url-pattern> </servlet-mapping>\"\n",
    "}\n",
    "'\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://35.167.139.79:9200/_analyze?pretty=true'\n",
    "response = requests.post(url, data=string)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "curl -XPUT 'http://35.167.139.79:9200/xml_index?pretty' -H 'Content-Type: application/json' -d'\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"xml_analyzer\": {\n",
    "          \"tokenizer\": \"xml_tokenizer\"\n",
    "        }\n",
    "      },\n",
    "      \"tokenizer\": {\n",
    "        \"xml_tokenizer\": {\n",
    "          \"type\": \"pattern\",\n",
    "          \"pattern\": \"<([^<>]+)>([^<>]*)<(\\/[^<>]+)>\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'\n",
    "\n",
    "curl -XPOST 'http://35.167.139.79:9200/xml_index/_analyze?pretty' -H 'Content-Type: application/json' -d'\n",
    "{\n",
    "  \"analyzer\": \"xml_analyzer\",\n",
    "  \"text\": \"<servlet-mapping> <servlet-name>HelloServlet</servlet-name> <url-pattern>/hello</url-pattern> </servlet-mapping>\"\n",
    "}\n",
    "'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
